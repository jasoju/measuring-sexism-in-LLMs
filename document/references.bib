
@article{samory_call_2021,
	title = {'{Call} me sexist, but...' : {Revisiting} {Sexism} {Detection} {Using} {Psychological} {Scales} and {Adversarial} {Samples}},
	volume = {15},
	copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
	issn = {2334-0770},
	shorttitle = {{\textquotedblleft}{Call} me sexist, but...{\textquotedblright}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/18085},
	doi = {10.1609/icwsm.v15i1.18085},
	abstract = {Research has focused on automated methods to effectively detect sexism online. Although overt sexism seems easy to spot, its subtle forms and manifold expressions are not. In this paper, we outline the different dimensions of sexism by grounding them in their implementation in psychological scales. From the scales, we derive a codebook for sexism in social media, which we use to annotate existing and novel datasets, surfacing their limitations in breadth and validity with respect to the construct of sexism. Next, we leverage the annotated datasets to generate adversarial examples, and test the reliability of sexism detection methods. Results indicate that current machine learning models pick up on a very narrow set of linguistic markers of sexism and do not generalize well to out-of-domain examples. Yet, including diverse data and adversarial examples at training time results in models that generalize better and that are more robust to artifacts of data collection. By providing a scale-based codebook and insights regarding the shortcomings of the state-of-the-art, we hope to contribute to the development of better and broader models for sexism detection, including reflections on theory-driven approaches to data collection.},
	urldate = {2024-07-23},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Samory, Mattia and Sen, Indira and Kohne, Julian and Fl{\"o}ck, Fabian and Wagner, Claudia},
	month = may,
	year = {2021},
	keywords = {psych ? AI, sexism detection},
	pages = {573--584},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\9B4SBAUY\\Samory et al. - 2021 - {\textquotedblleft}Call me sexist, but...{\textquotedblright}  Revisiting Sexism Detec.pdf:application/pdf},
}

@misc{stanczak_survey_2021,
	title = {A {Survey} on {Gender} {Bias} in {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2112.14168},
	doi = {10.48550/arXiv.2112.14168},
	abstract = {Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We suggest recommendations towards overcoming these limitations as a guide for future research.},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Stanczak, Karolina and Augenstein, Isabelle},
	month = dec,
	year = {2021},
	keywords = {criticism},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\VWCGNUNG\\Stanczak und Augenstein - 2021 - A Survey on Gender Bias in Natural Language Proces.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\L8H2ZHR3\\2112.html:text/html},
}

@mastersthesis{yun_bias_2023,
	title = {Bias in {Language} {Models}: {Defining}, {Measuring}, and {Reducing} {Bias}},
	author = {Yun, Chaewon},
	month = jun,
	year = {2023},
	keywords = {criticism},
	file = {Yun - Bias in Language Models Defining, Measuring, and .pdf:C\:\\Users\\jana\\Zotero\\storage\\4GJJWQBW\\Yun - Bias in Language Models Defining, Measuring, and .pdf:application/pdf},
}

@misc{akyurek_challenges_2022,
	title = {Challenges in {Measuring} {Bias} via {Open}-{Ended} {Language} {Generation}},
	url = {http://arxiv.org/abs/2205.11601},
	doi = {10.48550/arXiv.2205.11601},
	abstract = {Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups -- posing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under https://github.com/feyzaakyurek/bias-textgen.},
	urldate = {2024-07-23},
	publisher = {arXiv},
	author = {Aky{\"u}rek, Afra Feyza and Kocyigit, Muhammed Yusuf and Paik, Sejin and Wijaya, Derry},
	month = may,
	year = {2022},
	keywords = {not fitting?},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\5DTUUUC3\\Aky{\"u}rek et al. - 2022 - Challenges in Measuring Bias via Open-Ended Langua.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\Q48FG3Y7\\2205.html:text/html},
}

@inproceedings{jacobs_measurement_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Measurement and {Fairness}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445901},
	doi = {10.1145/3442188.3445901},
	abstract = {We propose measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as socioeconomic status, teacher effectiveness, and risk of recidivism. Such constructs cannot be measured directly and must instead be inferred from measurements of observable properties (and other unobservable theoretical constructs) thought to be related to them---i.e., operationalized via a measurement model. This process, which necessarily involves making assumptions, introduces the potential for mismatches between the theoretical understanding of the construct purported to be measured and its operationalization. We argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. We show how some of these harms could have been anticipated and, in some cases, mitigated if viewed through the lens of measurement modeling. To do this, we contribute fairness-oriented conceptualizations of construct reliability and construct validity that unite traditions from political science, education, and psychology and provide a set of tools for making explicit and testing assumptions about constructs and their operationalizations. We then turn to fairness itself, an essentially contested construct that has different theoretical understandings in different contexts. We argue that this contestedness underlies recent debates about fairness definitions: although these debates appear to be about different operationalizations, they are, in fact, debates about different theoretical understandings of fairness. We show how measurement modeling can provide a framework for getting to the core of these debates.},
	urldate = {2024-07-23},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Jacobs, Abigail Z. and Wallach, Hanna},
	year = {2021},
	keywords = {not fitting?},
	pages = {375--385},
	file = {Volltext:C\:\\Users\\jana\\Zotero\\storage\\Q3J9ANX3\\Jacobs und Wallach - 2021 - Measurement and Fairness.pdf:application/pdf},
}

@article{von_collani_ambivalent-sexistische_2003,
	title = {Ambivalent-sexistische {Einstellungen} gegen{\"u}ber {M{\"a}nnern} ({ASEM})},
	copyright = {Nutzungsbedingungen: Alle in ZIS dokumentierten Erhebungsinstrumente d{\"u}rfen kostenfrei f{\"u}r nicht-kommerzielle Forschungszwecke verwendet werden.Bei einem Einsatz f{\"u}r andere Zwecke oder in einer anderen als der hier dokumentierten Form ist das Einverst{\"a}ndnis der Autoren bzw. Autorinnen einzuholen. In allen resultierenden Arbeiten und Publikationen ist die ZIS-Dokumentation als Quelle anzugeben., Terms of use: All survey instruments documented in ZIS may be used free of charge for non-commercial research purposes. In the case of use for other purposes or in a form other than those documented here, the consent of the authors must be obtained. In all resulting works and publications, the ZIS documentation must be cited as the source.},
	url = {http://zis.gesis.org/DoiId/zis215},
	doi = {10.6102/ZIS215},
	abstract = {Die hier dokumentierten Items erfassen
ambivalent- sexistische Einstellungen gegen{\"u}ber M{\"a}nnern aus den beiden
Bereichen Hostilit{\"a}t (Feindseligkeit) und Benevolenz (F{\"u}rsorglichkeit). Diese
beiden Bereiche sind weiter untergliedert in jeweils drei Teilkomponenten
Paternalismus/ Maternalismus, Geschlechterdifferenzierung und Heterosexualit{\"a}t.
Sie operationalisieren damit das Konstrukt des ambivalenten Sexismus, nach dem
sexistische Einstellungen als positive Bewertungen (Gef{\"u}hle von N{\"a}he,
Geborgenheit, Zuwendung, Sicherheit) wie auch negative Einsch{\"a}tzungen
(Feindseligkeit, Wahrnehmung von Dominanz, Unterdr{\"u}ckung, Abh{\"a}ngigkeit)
zusammenh{\"a}ngen und gleichzeitig nebeneinander bestehen.},
	language = {de},
	urldate = {2024-09-17},
	journal = {Zusammenstellung sozialwissenschaftlicher Items und Skalen (ZIS)},
	author = {Von Collani, G. and Werner, R.},
	year = {2003},
	note = {Publisher: ZIS - GESIS Leibniz Institute for the Social Sciences
Version Number: 1.0},
	keywords = {german, men, ASI, OG scale},
	file = {Collani+_Ambivalent-sexistische_Einstellungen_gegenueber_Maennern_c.pdf:C\:\\Users\\jana\\Zotero\\storage\\GD4ZSPBY\\Collani+_Ambivalent-sexistische_Einstellungen_gegenueber_Maennern_c.pdf:application/pdf},
}

@article{glick_hostile_1997,
	title = {Hostile and {Benevolent} {Sexism}: {Measuring} {Ambivalent} {Sexist} {Attitudes} {Toward} {Women}},
	volume = {21},
	issn = {0361-6843},
	shorttitle = {Hostile and {Benevolent} {Sexism}},
	url = {https://doi.org/10.1111/j.1471-6402.1997.tb00104.x},
	doi = {10.1111/j.1471-6402.1997.tb00104.x},
	abstract = {A theory of sexism as ambivalence, not just hostility, toward women is presented. Ambivalent Sexism Theory distinguishes between hostile and {\textquotedblleft}benevolent{\textquotedblright} sexism (each addresses issues of power, gender differentiation, and sexuality). Benevolent sexism encompasses subjectively positive (for the sexist) attitudes toward women in traditional roles: protective paternalism, idealization of women, and desire for intimate relations. Hostile sexism encompasses the negative equivalents on each dimension: dominative paternalism, derogatory beliefs, and heterosexual hostility. Both forms of sexism serve to justify and maintain patriarchy and traditional gender roles. The validity of a measure of these constructs, the Ambivalent Sexism Inventory (ASI), is reviewed. Comparisons are offered between the ASI and other measures of sexist attitudes (e.g., the AWS), with suggestions for the proper domains of different scales.},
	number = {1},
	urldate = {2024-09-17},
	journal = {Psychology of Women Quarterly},
	author = {Glick, Peter and Fiske, Susan T.},
	month = mar,
	year = {1997},
	keywords = {women, english, ASI, OG scale},
	pages = {119--135},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\LS9B8CNY\\Glick und Fiske - 1997 - Hostile and Benevolent Sexism Measuring Ambivalen.pdf:application/pdf},
}

@inproceedings{perez_almendros_large_2024,
	address = {Torino, Italia},
	title = {Do {Large} {Language} {Models} {Understand} {Mansplaining}? {Well}, {Actually}...},
	shorttitle = {Do {Large} {Language} {Models} {Understand} {Mansplaining}?},
	url = {https://aclanthology.org/2024.lrec-main.466},
	abstract = {Gender bias has been widely studied by the NLP community. However, other more subtle variations of it, such as mansplaining, have yet received little attention. Mansplaining is a discriminatory behaviour that consists of a condescending treatment or discourse towards women. In this paper, we introduce and analyze Well, actually..., a corpus of 886 mansplaining stories experienced by women. We analyze the corpus in terms of features such as offensiveness, sentiment or misogyny, among others. We also explore to what extent Large Language Models (LLMs) can understand and identify mansplaining and other gender-related microaggressions. Specifically, we experiment with ChatGPT-3.5-Turbo and LLaMA-2 (13b and 70b), with both targeted and open questions. Our findings suggest that, although they can identify mansplaining to some extent, LLMs still struggle to point out this attitude and will even reproduce some of the social patterns behind mansplaining situations, for instance by praising men for giving unsolicited advice to women.},
	urldate = {2024-09-17},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Perez Almendros, Carla and Camacho-Collados, Jose},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {5235--5246},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\6ME94WF9\\Perez Almendros und Camacho-Collados - 2024 - Do Large Language Models Understand Mansplaining .pdf:application/pdf},
}

@article{eckes_hostilitat_1999,
	title = {Hostilit{\"a}t und {Benevolenz}: {Eine} {Skala} zur {Erfassung} des ambivalenten {Sexismus}},
	volume = {30},
	issn = {0044-3514},
	shorttitle = {Hostilit{\"a}t und {Benevolenz}},
	url = {https://econtent.hogrefe.com/doi/10.1024//0044-3514.30.4.211},
	doi = {10.1024//0044-3514.30.4.211},
	abstract = {Zusammenfassung: Es wird eine deutsche Version des {\guillemotleft}Ambivalent Sexism Inventory{\guillemotright} (ASI; Glick \& Fiske, 1996) vorgestellt. Diese Skala dient der Erfassung ambivalenter Einstellungen gegen{\"u}ber Frauen. Anders als die Konzepte des traditionellen und des modernen Sexismus trennt das Konzept des ambivalenten Sexismus zwischen negativen, offen feindseligen Einstellungen (Hostilit{\"a}t) und subjektiv positiven, wohlwollenden Einstellungen (Benevolenz). Untersuchungen an 2 studentischen und 3 nichtstudentischen Stichproben mit insgesamt 773 Vpn weisen die 22-Item-Skala als reliabel und valide aus. Insbesondere best{\"a}tigt sich die Hypothese, da{\ss} Hostilit{\"a}t und Benevolenz zwei separate Subsysteme sexistischer Einstellungen mit gegenl{\"a}ufiger subjektiver Valenz bilden. In der Diskussion werden Perspektiven der Forschung zu sexistischen Einstellungen aufgezeigt.},
	number = {4},
	urldate = {2024-09-17},
	journal = {Zeitschrift f{\"u}r Sozialpsychologie},
	author = {Eckes, Thomas and Six-Materna, Iris},
	month = dec,
	year = {1999},
	keywords = {german, women, ASI, OG scale},
	pages = {211--228},
}

@article{glick_ambivalence_1999,
	title = {The {Ambivalence} {Toward} {Men} {Inventory}},
	volume = {23},
	issn = {1471-6402},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1471-6402.1999.tb00379.x},
	doi = {10.1111/j.1471-6402.1999.tb00379.x},
	abstract = {We present a measure, the Ambivalence toward Men Inventory (AMI), that differentiates between women's hostile and benevolent prejudices and stereotypes about men. The Hostility toward Men (HM) and Benevolence toward Men (BM) subscales of the AMI tap conventional attitudes toward men that have opposing valences. Each subscale assesses subfactors concerning men's power, gender differentiation, and heterosexuality. Three studies with predominately White, male and female participants (two with undergraduates and one with a community sample) establish the factor structure, reliability, convergent validity, and predictive validity of the AMI. The AMI was strongly related to its sister scale, the Ambivalent Sexism Inventory (Glick \& Fiske, 1996) and to two established scales of attitudes toward men (Downs \& Engleson, 1982; Iazzo, 1983). Only the AMI, however, successfully distinguished between subjectively positive and subjectively negative beliefs about men. A copy of the 20-item AMI is provided as a tool for further exploration of women's ambivalence toward men.},
	number = {3},
	urldate = {2024-09-17},
	journal = {Psychology of Women Quarterly},
	author = {Glick, Peter and Fiske, Susan T.},
	year = {1999},
	keywords = {men, english, ASI, OG scale},
	pages = {519--536},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\N4Q57GAS\\Glick und Fiske - 1999 - The Ambivalence Toward Men Inventory.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\W5KY5983\\j.1471-6402.1999.tb00379.html:text/html},
}

@article{glick_ambivalent_2011,
	title = {Ambivalent {Sexism} {Revisited}},
	volume = {35},
	issn = {0361-6843},
	url = {https://doi.org/10.1177/0361684311414832},
	doi = {10.1177/0361684311414832},
	number = {3},
	urldate = {2024-09-17},
	journal = {Psychology of Women Quarterly},
	author = {Glick, Peter and Fiske, Susan T.},
	month = sep,
	year = {2011},
	pages = {530--535},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\NMQ6EVWY\\Glick und Fiske - 2011 - Ambivalent Sexism Revisited.pdf:application/pdf},
}

@article{glick_beyond_2000,
	title = {Beyond prejudice as simple antipathy: {Hostile} and benevolent sexism across cultures},
	volume = {79},
	issn = {1939-1315},
	shorttitle = {Beyond prejudice as simple antipathy},
	doi = {10.1037/0022-3514.79.5.763},
	abstract = {The authors argue that complementary hostile and benevolent components of sexism exist across cultures. Male dominance creates hostile sexism (HS), but men's dependence on women fosters benevolent sexism (BSx){\textemdash}subjectively positive attitudes that put women on a pedestal but reinforce their subordination. Research with 15,000 men and women in 19 nations showed that (1) HS and BSx are coherent constructs that correlate positively across nations, but (2) HS predicts the ascription of negative and BSx the ascription of positive traits to women, (3) relative to men, women are more likely to reject HS than BSx, especially when overall levels of sexism in a culture are high, and (4) national averages on BSx and HS predict gender inequality across nations. These results challenge prevailing notions of prejudice as an antipathy in that BSx (an affectionate, patronizing ideology) reflects inequality and is a cross-culturally pervasive complement to HS. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {5},
	journal = {Journal of Personality and Social Psychology},
	author = {Glick, Peter and Fiske, Susan T. and Mladinic, Antonio and Saiz, Jos{\'e} L. and Abrams, Dominic and Masser, Barbara and Adetoun, Bolanle and Osagie, Johnstone E. and Akande, Adebowale and Alao, Amos and Annetje, Barbara and Willemsen, Tineke M. and Chipeta, Kettie and Dardenne, Benoit and Dijksterhuis, Ap and Wigboldus, Daniel and Eckes, Thomas and Six-Materna, Iris and Exp{\'o}sito, Francisca and Moya, Miguel and Foddy, Margaret and Kim, Hyun-Jeong and Lameiras, Maria and Sotelo, Maria Jos{\'e} and Mucchi-Faina, Angelica and Romani, Myrna and Sakalli, Nuray and Udegbe, Bola and Yamamoto, Mariko and Ui, Miyoko and Ferreira, Maria Cristina and L{\'o}pez, Wilson L{\'o}pez},
	year = {2000},
	keywords = {ASI},
	pages = {763--775},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\MYBTYBTL\\2000-00920-007.html:text/html;Volltext:C\:\\Users\\jana\\Zotero\\storage\\XRMCYQM5\\Glick et al. - 2000 - Beyond prejudice as simple antipathy Hostile and .pdf:application/pdf},
}

@article{rollero_psychometric_2014,
	title = {Psychometric properties of {Short} {Versions} of the {Ambivalent} {Sexism} {Inventory} and {Ambivalence} {Toward} {Men} {Inventory}},
	url = {https://iris.unito.it/handle/2318/147212},
	doi = {10.4473/TPM21.2.3},
	abstract = {The current paper assesses the psychometric properties of short versions of the Ambivalent Sexism Inventory (ASI; Glick \& Fiske, 1996) and Ambivalence Toward Men Inventory (AMI; Glick \& Fiske, 1999), which represent widely used measures of sexist attitudes toward, respectively, women and men. Participants in the study were 960 Caucasian adults (48.6\% male). The theoretical structure of both the short ASI and AMI was tested via confirmatory factor analysis using structural equations modeling. Moreover, the invariance of the factor structures across gender and age was investigated. Results showed that the shorter versions of the ASI and AMI have good psychometric properties that are consistent with the original versions of the scales. Researchers who wish to assess ambivalent sexist attitudes, but must use fewer items than the original ASI and AMI, can strongly consider using these short versions.},
	urldate = {2024-09-23},
	author = {Rollero, Chiara and Peter, Glick and Tartaglia, Stefano},
	year = {2014},
	keywords = {ASI, short},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\98SU3HH3\\Rollero et al. - 2014 - Psychometric properties of Short Versions of the A.pdf:application/pdf},
}

@incollection{fiske_chapter_2015,
	address = {San Diego, California, USA},
	title = {Chapter 24 - {Measures} of {Stereotyping} and {Prejudice}: {Barometers} of {Bias}},
	isbn = {978-0-12-386915-9},
	shorttitle = {Chapter 24 - {Measures} of {Stereotyping} and {Prejudice}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123869159000243},
	abstract = {Twenty-first century intergroup biases are more automatic, ambivalent, and ambiguous than were old-fashioned biases such as authoritarianism and overt racism, which overtly expressed intergroup hostility. Beyond traditional self-report measures of ethnocentrism and hostile sexism, current measures tap more subtle manifestations of bias. Social dominance orientation assesses beliefs about the desirability of group hierarchies and predicts social attitudes such as ethnocentrism. The stereotype content model maps societal groups{\textquoteright} stereotypes, based on perceived social structure, predicting emotional prejudices and discriminatory tendencies. Recent racism measures tap modern policy-related attitude configurations, relatively automatic associations between groups and evaluations, and indirect indicators of intergroup attitudes. Current sexism scales assess modern versions oriented toward policies and an ambivalent version separating benevolence and hostility. Ageism scales measure both modern beliefs and prescriptive ambivalence toward older people. Current measures are less direct than earlier ones, consistent with 21st century patterns.},
	urldate = {2024-09-23},
	booktitle = {Measures of {Personality} and {Social} {Psychological} {Constructs}},
	publisher = {Academic Press},
	author = {Fiske, Susan T. and North, Michael S.},
	editor = {Boyle, Gregory J. and Saklofske, Donald H. and Matthews, Gerald},
	month = jan,
	year = {2015},
	keywords = {ASI, MSS, nomological network},
	pages = {684--718},
	file = {Fiske und North - 2015 - Chapter 24 - Measures of Stereotyping and Prejudic.pdf:C\:\\Users\\jana\\Zotero\\storage\\LFWHX7NC\\Fiske und North - 2015 - Chapter 24 - Measures of Stereotyping and Prejudic.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\Y6LQB6DA\\B9780123869159000243.html:text/html},
}

@article{trut_initial_2022,
	title = {Initial {Validation} of the {Ambivalent} {Sexism} {Inventory} in a {Military} {Setting}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-0760},
	url = {https://www.mdpi.com/2076-0760/11/4/176},
	doi = {10.3390/socsci11040176},
	abstract = {The military is a work environment in which the underrepresentation of women and the presence of gender prejudices continue to exist. The present study validated the Ambivalent Sexism Inventory (ASI) for the Croatian military population. To determine the ASI scale{\textquoteright}s basic metric characteristics, data were collected from a sample of 895 active-duty military personnel (445 men and 450 women). The study results determined satisfactory measurement characteristics for the ASI scale and confirmed the ambivalent sexism construct with its basic dimensions. Sexism in the military environment was found to a moderately high degree, and gender differences were observed. Three types of sexism endorsement were identified in both subsamples (egalitarian, moderate egalitarian and traditional for women, and moderate egalitarian, traditional and hostile for men), with additional differences detected in their socio{\textendash}demographic and professional characteristics. The findings support the apparent exposure of women to sexism in the military environment, and suggest the need to raise awareness of the negative impact of gender prejudice on gender relations in the military.},
	number = {4},
	urldate = {2024-09-23},
	journal = {Social Sciences},
	author = {Trut, Vesna and Sinov{\v c}i{\'c}, Petra and Milavi{\'c}, Boris},
	month = apr,
	year = {2022},
	keywords = {ASI},
	pages = {176},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\9HEMRQP9\\Trut et al. - 2022 - Initial Validation of the Ambivalent Sexism Invent.pdf:application/pdf},
}

@article{akrami_personality_2011,
	title = {Personality and {Social} {Psychology} {Factors} {Explaining} {Sexism}},
	volume = {32},
	issn = {1614-0001},
	url = {https://econtent.hogrefe.com/doi/abs/10.1027/1614-0001/a000043},
	doi = {10.1027/1614-0001/a000043},
	abstract = {Previous research has almost exclusively examined sexism (negative attitudes toward women) from either a personality or a social-psychology perspective. In two studies (N = 379 and 182, respectively), we combine these perspectives and examine whether sexism is best explained by personality (Big-Five factors, social dominance orientation, and right-wing authoritarianism) or by social-psychological (group membership and group identification) variables {\textendash} or by a combination of both approaches. Causal modeling and multiple regression analyses showed that, with the present set of variables, sexism was best explained by considering the combined influence of both personality- and social-psychology constructs. The findings imply that it is necessary to integrate various approaches to explain prejudice.},
	number = {3},
	urldate = {2024-09-23},
	journal = {Journal of Individual Differences},
	author = {Akrami, Nazar and Ekehammar, Bo and Yang-Wallentin, Fan},
	month = jan,
	year = {2011},
	pages = {153--160},
	file = {Akrami et al. - 2011 - Personality and Social Psychology Factors Explaini.pdf:C\:\\Users\\jana\\Zotero\\storage\\HYA73X6G\\Akrami et al. - 2011 - Personality and Social Psychology Factors Explaini.pdf:application/pdf},
}

@article{bareket_systematic_2023,
	title = {A systematic review of the ambivalent sexism literature: {Hostile} sexism protects men{\textquoteright}s power; benevolent sexism guards traditional gender roles},
	volume = {149},
	issn = {0033-2909},
	shorttitle = {A systematic review of the ambivalent sexism literature},
	url = {http://www.redi-bw.de/db/ebsco.php/search.ebscohost.com/login.aspx%3fdirect%3dtrue%26db%3dpdh%26AN%3d2024-16482-001%26site%3dehost-live},
	doi = {10.1037/bul0000400},
	abstract = {According to ambivalent sexism theory (Glick \& Fiske, 1996), the coexistence of gendered power differences and mutual interdependence creates two apparently opposing but complementary sexist ideologies: hostile sexism (HS; viewing women as manipulative competitors who seek to gain power over men) coincides with benevolent sexism (BS; a chivalrous view of women as pure and moral, yet weak and passive, deserving men{\textquoteright}s protection and admiration, as long as they conform). The research on these ideologies employs the Ambivalent Sexism Inventory, used extensively in psychology and allied disciplines, often to understand the roles sexist attitudes play in reinforcing gender inequality. Following contemporary guidelines, this systematic review utilizes a principled approach to synthesize the multidisciplinary empirical literature on ambivalent sexism. After screening 1,870 potentially relevant articles and fully reviewing 654 eligible articles, five main domains emerge in ambivalent sexism research (social ideologies, violence, workplace, stereotypes, intimate relationships). The accumulating evidence across domains offers bottom-up empirical support for ambivalent sexism as a coordinated system to maintain control over women (and sometimes men). Hostile sexism acts through the direct and diverse paths of envious/resentful prejudices, being more sensitive to power and sexuality cues; Benevolent sexism acts through prejudices related to interdependence (primarily gender-based paternalism and gender-role differentiation), enforcing traditional gender relations and being more sensitive to role-related cues. Discussion points to common methodological limitations, suggests guidelines, and finds future avenues for ambivalent sexism research. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {11-12},
	urldate = {2024-09-26},
	journal = {Psychological Bulletin},
	author = {Bareket, Orly and Fiske, Susan T.},
	month = nov,
	year = {2023},
	keywords = {important, nomological network},
	pages = {637--698},
	file = {EBSCO Full Text:C\:\\Users\\jana\\Zotero\\storage\\NF9K3NY6\\Bareket und Fiske - 2023 - A systematic review of the ambivalent sexism liter.pdf:application/pdf},
}

@article{hammond_benevolent_2018,
	title = {Benevolent {Sexism} and {Hostile} {Sexism} {Across} the {Ages}},
	volume = {9},
	issn = {1948-5506},
	url = {https://doi.org/10.1177/1948550617727588},
	doi = {10.1177/1948550617727588},
	abstract = {Ambivalent sexism theory states that prejudice toward women comprises two interrelated ideologies. Endorsement of hostile sexism{\textemdash}aggressive and competitive attitudes toward women{\textemdash}is linked with endorsement of benevolent sexism{\textemdash}paternalistic and patronizing attitudes toward women. We conduct the first systematic tests of how endorsement of sexism differs across age and across time, using six waves of a nationally representative panel sample of New Zealand adults (N = 10,398). Results indicated U-shaped trajectories for men{\textquoteright}s endorsement of hostile sexism, women{\textquoteright}s hostile sexism, and women{\textquoteright}s benevolent sexism across the life span. However, over time, endorsement of these sexist attitudes tended to decrease for most ages. In contrast, men{\textquoteright}s benevolent sexism followed a positive linear trajectory across age and tended not to change over time. These results provide novel evidence of how ambivalent sexism differs across age and highlight that benevolent sexism is particularly tenacious.},
	number = {7},
	urldate = {2024-09-26},
	journal = {Social Psychological and Personality Science},
	author = {Hammond, Matthew D. and Milojev, Petar and Huang, Yanshu and Sibley, Chris G.},
	month = sep,
	year = {2018},
	pages = {863--874},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\4ZUF9U7U\\Hammond et al. - 2018 - Benevolent Sexism and Hostile Sexism Across the Ag.pdf:application/pdf},
}

@inproceedings{kotek_gender_2023,
	address = {New York, NY, USA},
	series = {{CI} '23},
	title = {Gender bias and stereotypes in {Large} {Language} {Models}},
	isbn = {9798400701139},
	url = {https://dl.acm.org/doi/10.1145/3582269.3615599},
	doi = {10.1145/3582269.3615599},
	abstract = {Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs{\textquoteright} behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women{\textquoteright}s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person{\textquoteright}s gender; (b) these choices align with people{\textquoteright}s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95\% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably.},
	urldate = {2024-09-26},
	booktitle = {Proceedings of {The} {ACM} {Collective} {Intelligence} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
	month = nov,
	year = {2023},
	keywords = {occupational bias},
	pages = {12--24},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\IZCAMFRN\\Kotek et al. - 2023 - Gender bias and stereotypes in Large Language Mode.pdf:application/pdf},
}

@inproceedings{wan_kelly_2023,
	address = {Singapore},
	title = {{\textquotedblleft}{Kelly} is a {Warm} {Person}, {Joseph} is a {Role} {Model}{\textquotedblright}: {Gender} {Biases} in {LLM}-{Generated} {Reference} {Letters}},
	shorttitle = {{\textquotedblleft}{Kelly} is a {Warm} {Person}, {Joseph} is a {Role} {Model}{\textquotedblright}},
	url = {https://aclanthology.org/2023.findings-emnlp.243},
	doi = {10.18653/v1/2023.findings-emnlp.243},
	abstract = {Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs- ChatGPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.},
	urldate = {2024-09-26},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wan, Yixin and Pu, George and Sun, Jiao and Garimella, Aparna and Chang, Kai-Wei and Peng, Nanyun},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	keywords = {predictive validity},
	pages = {3730--3748},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\GRHGNU6N\\Wan et al. - 2023 - {\textquotedblleft}Kelly is a Warm Person, Joseph is a Role Model{\textquotedblright} .pdf:application/pdf},
}

@misc{kaneko_evaluating_2024,
	title = {Evaluating {Gender} {Bias} in {Large} {Language} {Models} via {Chain}-of-{Thought} {Prompting}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2401.15585},
	doi = {10.48550/ARXIV.2401.15585},
	abstract = {There exist both scalable tasks, like reading comprehension and fact-checking, where model performance improves with model size, and unscalable tasks, like arithmetic reasoning and symbolic reasoning, where model performance does not necessarily improve with model size. Large language models (LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate incremental predictions even on unscalable tasks. Unfortunately, despite their exceptional reasoning abilities, LLMs tend to internalize and reproduce discriminatory societal biases. Whether CoT can provide discriminatory or egalitarian rationalizations for the implicit information in unscalable tasks remains an open question.
 In this study, we examine the impact of LLMs' step-by-step predictions on gender bias in unscalable tasks. For this purpose, we construct a benchmark for an unscalable task where the LLM is given a list of words comprising feminine, masculine, and gendered occupational words, and is required to count the number of feminine and masculine words. In our CoT prompts, we require the LLM to explicitly indicate whether each word in the word list is a feminine or masculine before making the final predictions. With counting and handling the meaning of words, this benchmark has characteristics of both arithmetic reasoning and symbolic reasoning. Experimental results in English show that without step-by-step prediction, most LLMs make socially biased predictions, despite the task being as simple as counting words. Interestingly, CoT prompting reduces this unconscious social bias in LLMs and encourages fair predictions.},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Kaneko, Masahiro and Bollegala, Danushka and Okazaki, Naoaki and Baldwin, Timothy},
	year = {2024},
	keywords = {occupational bias},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\I329557W\\Kaneko et al. - 2024 - Evaluating Gender Bias in Large Language Models vi.pdf:application/pdf},
}

@inproceedings{blodgett_stereotyping_2021,
	address = {Online},
	title = {Stereotyping {Norwegian} {Salmon}: {An} {Inventory} of {Pitfalls} in {Fairness} {Benchmark} {Datasets}},
	shorttitle = {Stereotyping {Norwegian} {Salmon}},
	url = {https://aclanthology.org/2021.acl-long.81},
	doi = {10.18653/v1/2021.acl-long.81},
	abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens{\textemdash}originating from the social sciences{\textemdash}to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
	urldate = {2024-07-23},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	month = aug,
	year = {2021},
	keywords = {criticism},
	pages = {1004--1015},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\FFT99VW5\\Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf:application/pdf},
}

@misc{kotek_protected_2024,
	title = {Protected group bias and stereotypes in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2403.14727},
	doi = {10.48550/arXiv.2403.14727},
	abstract = {As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect {\textgreater}10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing responses that strongly emphasize diversity and equity to an extent that other group characteristics are overshadowed. This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should be applied in a careful and controlled manner.},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Kotek, Hadas and Sun, David Q. and Xiu, Zidi and Bowler, Margit and Klein, Christopher},
	month = mar,
	year = {2024},
	keywords = {occupational bias},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\EAI76UU9\\Kotek et al. - 2024 - Protected group bias and stereotypes in Large Lang.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\W2H267L3\\2403.html:text/html},
}

@inproceedings{kirk_bias_2021,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '21},
	title = {Bias out-of-the-box: an empirical analysis of intersectional occupational biases in popular generative language models},
	isbn = {978-1-71384-539-3},
	shorttitle = {Bias out-of-the-box},
	abstract = {The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as Hugging-Face have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied 'out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an indepth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities.},
	urldate = {2024-09-26},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Kirk, Hannah Rose and Jun, Yennie and Iqbal, Haider and Benussi, Elias and Volpin, Filippo and Dreyer, Frederic A. and Shtedritski, Aleksandar and Asano, Yuki M.},
	year = {2021},
	keywords = {occupational bias},
	pages = {2611--2624},
	file = {Kirk et al. - 2024 - Bias out-of-the-box an empirical analysis of inte.pdf:C\:\\Users\\jana\\Zotero\\storage\\EF4J5NWE\\Kirk et al. - 2024 - Bias out-of-the-box an empirical analysis of inte.pdf:application/pdf},
}

@article{gallegos_bias_2024,
	title = {Bias and {Fairness} in {Large} {Language} {Models}: {A} {Survey}},
	volume = {50},
	shorttitle = {Bias and {Fairness} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2024.cl-3.8},
	doi = {10.1162/coli_a_00524},
	abstract = {Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.},
	number = {3},
	urldate = {2024-09-26},
	journal = {Computational Linguistics},
	author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
	month = sep,
	year = {2024},
	pages = {1097--1179},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\75QHUGW8\\Gallegos et al. - 2024 - Bias and Fairness in Large Language Models A Surv.pdf:application/pdf},
}

@article{almeida_exploring_2024,
	title = {Exploring the psychology of {LLMs}{\textquoteright} moral and legal reasoning},
	volume = {333},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S000437022400081X},
	doi = {10.1016/j.artint.2024.104145},
	abstract = {Large language models (LLMs) exhibit expert-level performance in tasks across a wide range of different domains. Ethical issues raised by LLMs and the need to align future versions makes it important to know how state of the art models reason about moral and legal issues. In this paper, we employ the methods of experimental psychology to probe into this question. We replicate eight studies from the experimental literature with instances of Google's Gemini Pro, Anthropic's Claude 2.1, OpenAI's GPT-4, and Meta's Llama 2 Chat 70b. We find that alignment with human responses shifts from one experiment to another, and that models differ amongst themselves as to their overall alignment, with GPT-4 taking a clear lead over all other models we tested. Nonetheless, even when LLM-generated responses are highly correlated to human responses, there are still systematic differences, with a tendency for models to exaggerate effects that are present among humans, in part by reducing variance. This recommends caution with regards to proposals of replacing human participants with current state-of-the-art LLMs in psychological research and highlights the need for further research about the distinctive aspects of machine psychology.},
	urldate = {2024-09-30},
	journal = {Artificial Intelligence},
	author = {Almeida, Guilherme F. C. F. and Nunes, Jos{\'e} Luiz and Engelmann, Neele and Wiegmann, Alex and Ara{\'u}jo, Marcelo de},
	month = aug,
	year = {2024},
	keywords = {psych ? AI, machine psychology},
	pages = {104145},
	file = {ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\RGUKSADM\\S000437022400081X.html:text/html;Submitted Version:C\:\\Users\\jana\\Zotero\\storage\\8MZBWC3J\\Almeida et al. - 2024 - Exploring the psychology of LLMs{\textquoteright} moral and legal .pdf:application/pdf},
}

@misc{hagendorff_machine_2024,
	title = {Machine {Psychology}},
	url = {http://arxiv.org/abs/2303.13988},
	doi = {10.48550/arXiv.2303.13988},
	abstract = {Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains. Understanding their behavior and reasoning abilities therefore holds significant importance. We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior. In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table. It paves the way for a "machine psychology" for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs. We review existing work taking this approach, synthesize best practices, and highlight promising future directions. We also highlight the important caveats of applying methodologies designed for understanding humans to machines. We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Hagendorff, Thilo and Dasgupta, Ishita and Binz, Marcel and Chan, Stephanie C. Y. and Lampinen, Andrew and Wang, Jane X. and Akata, Zeynep and Schulz, Eric},
	month = aug,
	year = {2024},
	keywords = {psych ? AI, machine psychology},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\4YY3VFFV\\Hagendorff et al. - 2024 - Machine Psychology.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\975ICZHP\\2303.html:text/html},
}

@inproceedings{lohn_is_2024,
	address = {Tokyo, Japan},
	title = {Is {Machine} {Psychology} here? {On} {Requirements} for {Using} {Human} {Psychological} {Tests} on {Large} {Language} {Models}},
	shorttitle = {Is {Machine} {Psychology} here?},
	url = {https://aclanthology.org/2024.inlg-main.19},
	abstract = {In an effort to better understand the behavior of large language models (LLM), researchers recently turned to conducting psychological assessments on them. Several studies diagnose various psychological concepts in LLMs, such as psychopathological symptoms, personality traits, and intellectual functioning, aiming to unravel their black-box characteristics. But can we safely assess LLMs with tests that were originally designed for humans? The psychology domain looks back on decades of developing standards of appropriate testing procedures to ensure reliable and valid measures. We argue that analogous standardization processes are required for LLM assessments, given their differential functioning as compared to humans. In this paper, we propose seven requirements necessary for testing LLMs. Based on these, we critically reflect a sample of 25 recent machine psychology studies. Our analysis reveals (1) the lack of appropriate methods to assess test reliability and construct validity, (2) the unknown strength of construct-irrelevant influences, such as the contamination of pre-training corpora with test material, and (3) the pervasive issue of non-reproducibility of many studies. The results underscore the lack of a general methodology for the implementation of psychological assessments of LLMs and the need to redefine psychological constructs specifically for large language models rather than adopting them from human psychology.},
	urldate = {2024-09-30},
	booktitle = {Proceedings of the 17th {International} {Natural} {Language} {Generation} {Conference}},
	publisher = {Association for Computational Linguistics},
	author = {L{\"o}hn, Lea and Kiehne, Niklas and Ljapunov, Alexander and Balke, Wolf-Tilo},
	editor = {Mahamood, Saad and Minh, Nguyen Le and Ippolito, Daphne},
	month = sep,
	year = {2024},
	keywords = {important, psych ? AI, machine psychology},
	pages = {230--242},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\CIFWGLWS\\L{\"o}hn et al. - 2024 - Is Machine Psychology here On Requirements for Us.pdf:application/pdf},
}

@article{stella_using_2023,
	title = {Using cognitive psychology to understand {GPT}-like models needs to extend beyond human biases},
	volume = {120},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2312911120},
	doi = {10.1073/pnas.2312911120},
	number = {43},
	urldate = {2024-09-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Stella, Massimo and Hills, Thomas T. and Kenett, Yoed N.},
	month = oct,
	year = {2023},
	keywords = {psych ? AI},
	pages = {e2312911120},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\AEA98DQM\\Stella et al. - 2023 - Using cognitive psychology to understand GPT-like .pdf:application/pdf},
}

@article{pellert_ai_2024,
	title = {{AI} {Psychometrics}: {Assessing} the {Psychological} {Profiles} of {Large} {Language} {Models} {Through} {Psychometric} {Inventories}},
	volume = {19},
	issn = {1745-6916},
	shorttitle = {{AI} {Psychometrics}},
	url = {https://doi.org/10.1177/17456916231214460},
	doi = {10.1177/17456916231214460},
	abstract = {We illustrate how standard psychometric inventories originally designed for assessing noncognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (LLMs). We start from the assumption that LLMs, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. Such corpora contain sediments of the personalities, values, beliefs, and biases of the countless human authors of these texts, which LLMs learn through a complex training process. The traits that LLMs acquire in such a way can potentially influence their behavior, that is, their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. By eliciting LLMs{\textquoteright} responses to language-based psychometric inventories, we can bring their traits to light. Psychometric profiling enables researchers to study and compare LLMs in terms of noncognitive characteristics, thereby providing a window into the personalities, values, beliefs, and biases these models exhibit (or mimic). We discuss the history of similar ideas and outline possible psychometric approaches for LLMs. We demonstrate one promising approach, zero-shot classification, for several LLMs and psychometric inventories. We conclude by highlighting open challenges and future avenues of research for AI Psychometrics.},
	number = {5},
	urldate = {2024-09-30},
	journal = {Perspectives on Psychological Science},
	author = {Pellert, Max and Lechner, Clemens M. and Wagner, Claudia and Rammstedt, Beatrice and Strohmaier, Markus},
	month = sep,
	year = {2024},
	keywords = {psych ? AI, machine psychology},
	pages = {808--826},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\AFI5VU5S\\Pellert et al. - 2024 - AI Psychometrics Assessing the Psychological Prof.pdf:application/pdf},
}

@article{bail_can_2024,
	title = {Can {Generative} {AI} improve social science?},
	volume = {121},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2314021121},
	doi = {10.1073/pnas.2314021121},
	abstract = {Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research{\textemdash}as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.},
	number = {21},
	urldate = {2024-09-30},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bail, Christopher A.},
	month = may,
	year = {2024},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	keywords = {psych ? AI, silicon sampling},
	pages = {e2314021121},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\JRP33PRB\\Bail - 2024 - Can Generative AI improve social science.pdf:application/pdf},
}

@article{argyle_out_2023,
	title = {Out of {One}, {Many}: {Using} {Language} {Models} to {Simulate} {Human} {Samples}},
	volume = {31},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Out of {One}, {Many}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/out-of-one-many-using-language-models-to-simulate-human-samples/035D7C8A55B237942FB6DBAD7CAA4E49},
	doi = {10.1017/pan.2023.2},
	abstract = {We propose and explore the possibility that language models can be studied as effective proxies for specific human subpopulations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the {\textquotedblleft}algorithmic bias{\textquotedblright} within one such tool{\textemdash}the GPT-3 language model{\textemdash}is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create {\textquotedblleft}silicon samples{\textquotedblright} by conditioning the model on thousands of sociodemographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and sociocultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.},
	number = {3},
	urldate = {2024-09-30},
	journal = {Political Analysis},
	author = {Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
	month = jul,
	year = {2023},
	keywords = {silicon sampling},
	pages = {337--351},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\ZIC49YTU\\Argyle et al. - 2023 - Out of One, Many Using Language Models to Simulat.pdf:application/pdf},
}

@article{bisbee_synthetic_2024,
	title = {Synthetic {Replacements} for {Human} {Survey} {Data}? {The} {Perils} of {Large} {Language} {Models}},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Synthetic {Replacements} for {Human} {Survey} {Data}?},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/synthetic-replacements-for-human-survey-data-the-perils-of-large-language-models/B92267DC26195C7F36E63EA04A47D2FE},
	doi = {10.1017/pan.2024.5},
	abstract = {Large language models (LLMs) offer new research possibilities for social scientists, but their potential as {\textquotedblleft}synthetic data{\textquotedblright} is still largely unknown. In this paper, we investigate how accurately the popular LLM ChatGPT can recover public opinion, prompting the LLM to adopt different {\textquotedblleft}personas{\textquotedblright} and then provide feeling thermometer scores for 11 sociopolitical groups. The average scores generated by ChatGPT correspond closely to the averages in our baseline survey, the 2016{\textendash}2020 American National Election Study (ANES). Nevertheless, sampling by ChatGPT is not reliable for statistical inference: there is less variation in responses than in the real surveys, and regression coefficients often differ significantly from equivalent estimates obtained using ANES data. We also document how the distribution of synthetic responses varies with minor changes in prompt wording, and we show how the same prompt yields significantly different results over a 3-month period. Altogether, our findings raise serious concerns about the quality, reliability, and reproducibility of synthetic survey data generated by LLMs.},
	urldate = {2024-09-30},
	journal = {Political Analysis},
	author = {Bisbee, James and Clinton, Joshua D. and Dorff, Cassy and Kenkel, Brenton and Larson, Jennifer M.},
	month = may,
	year = {2024},
	keywords = {silicon sampling},
	pages = {1--16},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\ULVRE6TT\\Bisbee et al. - 2024 - Synthetic Replacements for Human Survey Data The .pdf:application/pdf},
}

@misc{santurkar_whose_2023,
	title = {Whose {Opinions} {Do} {Language} {Models} {Reflect}?},
	url = {http://arxiv.org/abs/2303.17548},
	doi = {10.48550/arXiv.2303.17548},
	abstract = {Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions\_qa.},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
	month = mar,
	year = {2023},
	note = {arXiv:2303.17548 [cs]},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\6UI8ZHTL\\Santurkar et al. - 2023 - Whose Opinions Do Language Models Reflect.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\G2XSC4JW\\2303.html:text/html},
}

@misc{coda-forno_inducing_2023,
	title = {Inducing anxiety in large language models increases exploration and bias},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2304.11111},
	doi = {10.48550/ARXIV.2304.11111},
	abstract = {Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a strong increase in biases when prompted with anxiety-inducing text. Thus, it is likely that how prompts are communicated to large language models has a strong influence on their behavior in applied settings. These results progress our understanding of prompt engineering and demonstrate the usefulness of methods taken from computational psychiatry for studying the capable algorithms to which we increasingly delegate authority and autonomy.},
	urldate = {2024-10-08},
	publisher = {arXiv},
	author = {Coda-Forno, Julian and Witte, Kristin and Jagadish, Akshay K. and Binz, Marcel and Akata, Zeynep and Schulz, Eric},
	year = {2023},
	keywords = {psych ? AI, machine psychology, individual},
	file = {Coda-Forno et al. - 2023 - Inducing anxiety in large language models increase.pdf:C\:\\Users\\jana\\Zotero\\storage\\GB8T6C88\\Coda-Forno et al. - 2023 - Inducing anxiety in large language models increase.pdf:application/pdf},
}

@misc{serapio-garcia_personality_2023,
	title = {Personality {Traits} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2307.00184},
	doi = {10.48550/ARXIV.2307.00184},
	abstract = {The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.},
	urldate = {2024-10-08},
	publisher = {arXiv},
	author = {Serapio-Garc{\'i}a, Greg and Safdari, Mustafa and Crepy, Cl{\'e}ment and Sun, Luning and Fitz, Stephen and Romero, Peter and Abdulhai, Marwa and Faust, Aleksandra and Matari{\'c}, Maja},
	year = {2023},
	keywords = {important, psych ? AI, machine psychology, answer extraction, population},
	file = {Serapio-Garc{\'i}a et al. - 2023 - Personality Traits in Large Language Models.pdf:C\:\\Users\\jana\\Zotero\\storage\\T46PW9FH\\Serapio-Garc{\'i}a et al. - 2023 - Personality Traits in Large Language Models.pdf:application/pdf},
}

@inproceedings{miotto_who_2022,
	address = {Abu Dhabi, UAE},
	title = {Who is {GPT}-3? {An} exploration of personality, values and demographics},
	shorttitle = {Who is {GPT}-3?},
	url = {https://aclanthology.org/2022.nlpcss-1.24},
	doi = {10.18653/v1/2022.nlpcss-1.24},
	abstract = {Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.},
	urldate = {2024-10-08},
	booktitle = {Proceedings of the {Fifth} {Workshop} on {Natural} {Language} {Processing} and {Computational} {Social} {Science} ({NLP}+{CSS})},
	publisher = {Association for Computational Linguistics},
	author = {Miotto, Maril{\`u} and Rossberg, Nicola and Kleinberg, Bennett},
	editor = {Bamman, David and Hovy, Dirk and Jurgens, David and Keith, Katherine and O'Connor, Brendan and Volkova, Svitlana},
	month = nov,
	year = {2022},
	keywords = {psych ? AI, machine psychology, individual},
	pages = {218--227},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\HTCT5ZK4\\Miotto et al. - 2022 - Who is GPT-3 An exploration of personality, value.pdf:application/pdf},
}

@misc{fischer_what_2023,
	title = {What does {ChatGPT} return about human values? {Exploring} value bias in {ChatGPT} using a descriptive value theory},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {What does {ChatGPT} return about human values?},
	url = {https://arxiv.org/abs/2304.03612},
	doi = {10.48550/ARXIV.2304.03612},
	abstract = {There has been concern about ideological basis and possible discrimination in text generated by Large Language Models (LLMs). We test possible value biases in ChatGPT using a psychological value theory. We designed a simple experiment in which we used a number of different probes derived from the Schwartz basic value theory (items from the revised Portrait Value Questionnaire, the value type definitions, value names). We prompted ChatGPT via the OpenAI API repeatedly to generate text and then analyzed the generated corpus for value content with a theory-driven value dictionary using a bag of words approach. Overall, we found little evidence of explicit value bias. The results showed sufficient construct and discriminant validity for the generated text in line with the theoretical predictions of the psychological model, which suggests that the value content was carried through into the outputs with high fidelity. We saw some merging of socially oriented values, which may suggest that these values are less clearly differentiated at a linguistic level or alternatively, this mixing may reflect underlying universal human motivations. We outline some possible applications of our findings for both applications of ChatGPT for corporate usage and policy making as well as future research avenues. We also highlight possible implications of this relatively high-fidelity replication of motivational content using a linguistic model for the theorizing about human values.},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Fischer, Ronald and Luczak-Roesch, Markus and Karl, Johannes A.},
	year = {2023},
	keywords = {psych ? AI, machine psychology},
	file = {Fischer et al. - 2023 - What does ChatGPT return about human values Explo.pdf:C\:\\Users\\jana\\Zotero\\storage\\FJ2R4EYX\\Fischer et al. - 2023 - What does ChatGPT return about human values Explo.pdf:application/pdf},
}

@misc{wang_look_2024,
	title = {Look at the {Text}: {Instruction}-{Tuned} {Language} {Models} are {More} {Robust} {Multiple} {Choice} {Selectors} than {You} {Think}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Look at the {Text}},
	url = {https://arxiv.org/abs/2404.08382},
	doi = {10.48550/ARXIV.2404.08382},
	abstract = {Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50{\textbackslash}\%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation.},
	urldate = {2024-10-09},
	publisher = {arXiv},
	author = {Wang, Xinpeng and Hu, Chengzhi and Ma, Bolei and R{\"o}ttger, Paul and Plank, Barbara},
	year = {2024},
	keywords = {answer extraction},
	file = {Wang et al. - 2024 - Look at the Text Instruction-Tuned Language Model.pdf:C\:\\Users\\jana\\Zotero\\storage\\BY3THDF8\\Wang et al. - 2024 - Look at the Text Instruction-Tuned Language Model.pdf:application/pdf},
}

@misc{kovac_large_2023,
	title = {Large {Language} {Models} as {Superpositions} of {Cultural} {Perspectives}},
	url = {http://arxiv.org/abs/2307.07870},
	doi = {10.48550/arXiv.2307.07870},
	abstract = {Large Language Models (LLMs) are often misleadingly recognized as having a personality or a set of values. We argue that an LLM can be seen as a superposition of perspectives with different values and personality traits. LLMs exhibit context-dependent values and personality traits that change based on the induced perspective (as opposed to humans, who tend to have more coherent values and personality traits across contexts). We introduce the concept of perspective controllability, which refers to a model's affordance to adopt various perspectives with differing values and personality traits. In our experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study how exhibited values and personality traits change based on different perspectives. Through qualitative experiments, we show that LLMs express different values when those are (implicitly or explicitly) implied in the prompt, and that LLMs express different values even when those are not obviously implied (demonstrating their context-dependent nature). We then conduct quantitative experiments to study the controllability of different models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the effectiveness of various methods for inducing perspectives, and the smoothness of the models' drivability. We conclude by examining the broader implications of our work and outline a variety of associated scientific questions. The project website is available at https://sites.google.com/view/llm-superpositions .},
	urldate = {2024-10-13},
	publisher = {arXiv},
	author = {Kova{\v c}, Grgur and Sawayama, Masataka and Portelas, R{\'e}my and Colas, C{\'e}dric and Dominey, Peter Ford and Oudeyer, Pierre-Yves},
	month = nov,
	year = {2023},
	keywords = {psych ? AI},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\UN6Y7AB5\\Kova{\v c} et al. - 2023 - Large Language Models as Superpositions of Cultura.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\ZGHJZNUI\\2307.html:text/html},
}

@book{allport_nature_1954,
	address = {Reading, Mass., USA},
	title = {The nature of prejudice},
	isbn = {978-0-201-00175-4},
	abstract = {23 cm. - Includes bibliography.},
	publisher = {Addison-Wesley},
	author = {Allport, Gordon W.},
	year = {1954},
}

@article{glick_ambivalent_2001,
	title = {An ambivalent alliance: {Hostile} and benevolent sexism as complementary justifications for gender inequality},
	volume = {56},
	issn = {1935-990X},
	shorttitle = {An ambivalent alliance},
	doi = {10.1037/0003-066X.56.2.109},
	abstract = {The equation of prejudice with antipathy is challenged by recent research on sexism. Benevolent sexism (a subjectively favorable, chivalrous ideology that offers protection and affection to women who embrace conventional roles) coexists with hostile sexism (antipathy toward women who are viewed as usurping men's power). The Ambivalent Sexism Inventory, first validated in U.S. samples, has been administered to over 15,000 men and women in 19 nations. Hostile and benevolent sexism are complementary, cross-culturally prevalent ideologies, both of which predict gender inequality. Women, as compared with men, consistently reject hostile sexism but often endorse benevolent sexism (especially in the most sexist cultures). By rewarding women for conforming to a patriarchal status quo, benevolent sexism inhibits gender equality. More generally, affect toward minority groups is often ambivalent, but subjectively positive stereotypes are not necessarily benign. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {2},
	journal = {American Psychologist},
	author = {Glick, Peter and Fiske, Susan T.},
	year = {2001},
	pages = {109--118},
	file = {Glick and Fiske - 2001 - An ambivalent alliance Hostile and benevolent sex.pdf:C\:\\Users\\jana\\Zotero\\storage\\LC7UPKNB\\Glick and Fiske - 2001 - An ambivalent alliance Hostile and benevolent sex.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\8VTZQSM6\\doiLanding.html:text/html},
}

@article{fiske_prejudices_2017,
	title = {Prejudices in {Cultural} {Contexts}: {Shared} {Stereotypes} ({Gender}, {Age}) {Versus} {Variable} {Stereotypes} ({Race}, {Ethnicity}, {Religion})},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Prejudices in {Cultural} {Contexts}},
	url = {https://doi.org/10.1177/1745691617708204},
	doi = {10.1177/1745691617708204},
	abstract = {Some prejudices share cross-cultural patterns, but others are more variable and culture specific. Those sharing cross-cultural patterns (sexism, ageism) each combine societal status differences and intimate interdependence. For example, in stereotypes of sex and age, lower status groups{\textemdash}women and elders{\textemdash}gain stereotypic warmth (from their cooperative interdependence) but lose stereotypic competence (from their lower status); men and middle-aged adults show the opposite trade-off, stereotypically more competent than warm. Meta-analyses support these widespread ambivalent (mixed) stereotypes for gender and age across cultures. Social class stereotypes often share some similarities (cold but competent rich vs. warm but incompetent poor). These compensatory warmth versus competence stereotypes may function to manage common human dilemmas of interacting across societal and personal positions. However, other stereotypes are more variable and culture specific (ethnicity, race, religion). Case studies of specific race/ethnicities and religions reveal much more cultural variation in their stereotype content, supporting their being responses to particular cultural contexts, apparent accidents of history. To change stereotypes requires understanding their commonalities and differences, their origins and patterns across cultures.},
	number = {5},
	urldate = {2024-10-14},
	journal = {Perspectives on Psychological Science},
	author = {Fiske, Susan T.},
	month = sep,
	year = {2017},
	pages = {791--799},
	file = {Accepted Version:C\:\\Users\\jana\\Zotero\\storage\\YFFDKX9W\\Fiske - 2017 - Prejudices in Cultural Contexts Shared Stereotype.pdf:application/pdf},
}

@article{van_der_toorn_not_2020,
	series = {Political {Ideologies}},
	title = {Not quite over the rainbow: the unrelenting and insidious nature of heteronormative ideology},
	volume = {34},
	issn = {2352-1546},
	shorttitle = {Not quite over the rainbow},
	url = {https://www.sciencedirect.com/science/article/pii/S2352154620300383},
	doi = {10.1016/j.cobeha.2020.03.001},
	abstract = {Heteronormative ideology refers to the belief that there are two separate and opposing genders with associated natural roles that match their assigned sex, and that heterosexuality is a given. It is pervasive and persistent, carrying negative consequences. Because it is embedded in societal institutions and propagated through socialization and other widely held ideologies, it is prevalent among both cis-hetero and LGBTQI+ individuals. In the current article, we discuss the unrelenting and insidious nature of heteronormative ideology, review some of the social-psychological mechanisms that contribute to its maintenance, and provide directions for future research that could inform efforts to combat it. We argue that threat reactions to non-heteronormative behavior reinforce heteronormative beliefs and that interventions are needed to address both prejudice and its underlying mechanisms.},
	urldate = {2024-10-15},
	journal = {Current Opinion in Behavioral Sciences},
	author = {van der Toorn, Jojanneke and Pliskin, Ruthie and Morgenroth, Thekla},
	month = aug,
	year = {2020},
	pages = {160--165},
	file = {Full Text:C\:\\Users\\jana\\Zotero\\storage\\Y4262D9Q\\van der Toorn et al. - 2020 - Not quite over the rainbow the unrelenting and in.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\4PJ73QM3\\S2352154620300383.html:text/html;van der Toorn et al. - 2020 - Not quite over the rainbow the unrelenting and in.pdf:C\:\\Users\\jana\\Zotero\\storage\\7HZGDYQB\\van der Toorn et al. - 2020 - Not quite over the rainbow the unrelenting and in.pdf:application/pdf},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2024-10-16},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	year = {2021},
	pages = {610--623},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\22YI7DWC\\Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@misc{webster_measuring_2020,
	title = {Measuring and {Reducing} {Gendered} {Correlations} in {Pre}-trained {Models}},
	url = {https://ui.adsabs.harvard.edu/abs/2020arXiv201006032W},
	doi = {10.48550/arXiv.2010.06032},
	abstract = {Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.},
	language = {en},
	urldate = {2023-10-18},
	author = {Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Chi, Ed and Petrov, Slav},
	month = oct,
	year = {2020},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2020arXiv201006032W},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\7PPB6XIP\\Webster et al. - 2020 - Measuring and Reducing Gendered Correlations in Pr.pdf:application/pdf},
}

@article{garg_word_2018,
	title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
	volume = {115},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1720347115},
	doi = {10.1073/pnas.1720347115},
	abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts{\textemdash}e.g., the women{\textquoteright}s movement in the 1960s and Asian immigration into the United States{\textemdash}and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.},
	language = {en},
	number = {16},
	urldate = {2024-01-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Garg, Nikhil and Schiebinger, Londa and Jurafsky, Dan and Zou, James},
	month = apr,
	year = {2018},
	pages = {E3635--E3644},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\T9VBR35L\\Garg et al. - 2018 - Word embeddings quantify 100 years of gender and e.pdf:application/pdf},
}

@misc{apa_dictionary_of_psychology_gender_2023,
	title = {Gender bias},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2024-10-16},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = nov,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\EQI2QPEE\\gender-bias.html:text/html},
}

@inproceedings{blodgett_language_2020,
	address = {Online},
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of {\textquotedblleft}{Bias}{\textquotedblright} in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {https://aclanthology.org/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	abstract = {We survey 146 papers analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {\textquotedblleft}bias{\textquotedblright} is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating {\textquotedblleft}bias{\textquotedblright} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {\textquotedblleft}bias{\textquotedblright} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {\textquotedblleft}bias{\textquotedblright}{\textemdash}i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{\textemdash}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2024-10-16},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	keywords = {criticism},
	pages = {5454--5476},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\ZEQWB542\\Blodgett et al. - 2020 - Language (Technology) is Power A Critical Survey .pdf:application/pdf},
}

@article{caliskan_semantics_2017,
	title = {Semantics derived automatically from language corpora contain human-like biases},
	volume = {356},
	url = {https://www.science.org/doi/10.1126/science.aal4230},
	doi = {10.1126/science.aal4230},
	abstract = {Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
	number = {6334},
	urldate = {2023-10-11},
	journal = {Science},
	author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
	month = apr,
	year = {2017},
	keywords = {occupational bias},
	pages = {183--186},
	file = {Accepted Version:C\:\\Users\\jana\\Zotero\\storage\\675T2KQP\\Caliskan et al. - 2017 - Semantics derived automatically from language corp.pdf:application/pdf},
}

@inproceedings{zhao_gender_2018,
	address = {New Orleans, Louisiana, USA},
	title = {Gender {Bias} in {Coreference} {Resolution}: {Evaluation} and {Debiasing} {Methods}},
	shorttitle = {Gender {Bias} in {Coreference} {Resolution}},
	url = {http://arxiv.org/abs/1804.06876},
	doi = {10.18653/v1/N18-2003},
	abstract = {We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.},
	urldate = {2023-10-08},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
	month = apr,
	year = {2018},
	keywords = {occupational bias},
	pages = {15--20},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\443KYYUQ\\Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\LBU3X8WB\\1804.html:text/html},
}

@inproceedings{parrish_bbq_2022,
	address = {Dublin, Ireland},
	title = {{BBQ}: {A} hand-built bias benchmark for question answering},
	shorttitle = {{BBQ}},
	url = {https://aclanthology.org/2022.findings-acl.165},
	doi = {10.18653/v1/2022.findings-acl.165},
	abstract = {It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model's biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model's outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.},
	urldate = {2024-10-17},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Parrish, Alicia and Chen, Angelica and Nangia, Nikita and Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and Htut, Phu Mon and Bowman, Samuel},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {2086--2105},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\CWJK2M25\\Parrish et al. - 2022 - BBQ A hand-built bias benchmark for question answ.pdf:application/pdf},
}

@inproceedings{green_good_2019,
	title = {"{Good}" isn{\textquoteright}t good enough},
	volume = {17},
	url = {https://aiforsocialgood.github.io/neurips2019/accepted/track3/pdfs/67_aisg_neurips2019.pdf},
	urldate = {2024-10-17},
	booktitle = {Proceedings of the {AI} for {Social} {Good} workshop at {NeurIPS}},
	author = {Green, Ben},
	year = {2019},
	file = {Available Version (via Google Scholar):C\:\\Users\\jana\\Zotero\\storage\\JPLH7XR6\\Green - 2019 - Good{\textquotedblright} isn{\textquoteright}t good enough.pdf:application/pdf},
}

@inproceedings{waseem_hateful_2016,
	title = {Hateful symbols or hateful people? predictive features for hate speech detection on twitter},
	shorttitle = {Hateful symbols or hateful people?},
	url = {https://aclanthology.org/N16-2013.pdf},
	urldate = {2024-10-17},
	booktitle = {Proceedings of the {NAACL} student research workshop},
	author = {Waseem, Zeerak and Hovy, Dirk},
	year = {2016},
	pages = {88--93},
	file = {Available Version (via Google Scholar):C\:\\Users\\jana\\Zotero\\storage\\PSFUBKC3\\Waseem and Hovy - 2016 - Hateful symbols or hateful people predictive feat.pdf:application/pdf},
}

@article{tavarez-rodriguez_better_2024,
	title = {Better together: {LLM} and neural classification transformers to detect sexism},
	shorttitle = {Better together},
	url = {https://ceur-ws.org/Vol-3740/paper-118.pdf},
	urldate = {2024-10-17},
	journal = {Working Notes of CLEF},
	author = {Tavarez-Rodr{\'i}guez, Judith and S{\'a}nchez-Vega, Fernando and Rosales-P{\'e}rez, Alejandro and L{\'o}pez-Monroy, Adri{\'a}n Pastor},
	year = {2024},
	file = {Available Version (via Google Scholar):C\:\\Users\\jana\\Zotero\\storage\\CSFLRRV2\\Tavarez-Rodr{\'i}guez et al. - 2024 - Better together LLM and neural classification tra.pdf:application/pdf},
}

@inproceedings{grosz_automatic_2020,
	address = {Cham, Switzerland},
	title = {Automatic {Detection} of {Sexist} {Statements} {Commonly} {Used} at the {Workplace}},
	isbn = {978-3-030-60470-7},
	doi = {10.1007/978-3-030-60470-7_11},
	abstract = {Detecting hate speech in the workplace is a unique classification task, as the underlying social context implies a subtler version of conventional hate speech. Applications regarding a state-of-the-art workplace sexism detection model include aids for Human Resources departments, AI chatbots and sentiment analysis. Most existing hate speech detection methods, although robust and accurate, focus on hate speech found on social media, specifically Twitter. The context of social media is much more anonymous than the workplace, therefore it tends to lend itself to more aggressive and {\textquotedblleft}hostile{\textquotedblright} versions of sexism. Therefore, datasets with large amounts of {\textquotedblleft}hostile{\textquotedblright} sexism have a slightly easier detection task since {\textquotedblleft}hostile{\textquotedblright} sexist statements can hinge on a couple words that, regardless of context, tip the model off that a statement is sexist. In this paper we present a dataset of sexist statements that are more likely to be said in the workplace as well as a deep learning model that can achieve state-of-the art results. Previous research has created state-of-the-art models to distinguish {\textquotedblleft}hostile{\textquotedblright} and {\textquotedblleft}benevolent{\textquotedblright} sexism based simply on aggregated Twitter data. Our deep learning methods, initialized with GloVe or random word embeddings, use LSTMs with attention mechanisms to outperform those models on a more diverse, filtered dataset that is more targeted towards workplace sexism, leading to an F1 score of 0.88.},
	booktitle = {Trends and {Applications} in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Grosz, Dylan and Conde-Cespedes, Patricia},
	editor = {Lu, Wei and Zhu, Kenny Q.},
	year = {2020},
	keywords = {sexism detection},
	pages = {104--115},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\Z44UX576\\Grosz and Conde-Cespedes - 2020 - Automatic Detection of Sexist Statements Commonly .pdf:application/pdf},
}

@misc{schutz_automatic_2022,
	title = {Automatic {Sexism} {Detection} with {Multilingual} {Transformer} {Models}},
	url = {http://arxiv.org/abs/2106.04908},
	doi = {10.48550/arXiv.2106.04908},
	abstract = {Sexism has become an increasingly major problem on social networks during the last years. The first shared task on sEXism Identification in Social neTworks (EXIST) at IberLEF 2021 is an international competition in the field of Natural Language Processing (NLP) with the aim to automatically identify sexism in social media content by applying machine learning methods. Thereby sexism detection is formulated as a coarse (binary) classification problem and a fine-grained classification task that distinguishes multiple types of sexist content (e.g., dominance, stereotyping, and objectification). This paper presents the contribution of the AIT\_FHSTP team at the EXIST2021 benchmark for both tasks. To solve the tasks we applied two multilingual transformer models, one based on multilingual BERT and one based on XLM-R. Our approach uses two different strategies to adapt the transformers to the detection of sexist content: first, unsupervised pre-training with additional data and second, supervised fine-tuning with additional and augmented data. For both tasks our best model is XLM-R with unsupervised pre-training on the EXIST data and additional datasets and fine-tuning on the provided dataset. The best run for the binary classification (task 1) achieves a macro F1-score of 0.7752 and scores 5th rank in the benchmark; for the multiclass classification (task 2) our best submission scores 6th rank with a macro F1-score of 0.5589.},
	urldate = {2024-10-17},
	publisher = {arXiv},
	author = {Sch{\"u}tz, Mina and Boeck, Jaqueline and Liakhovets, Daria and Slijep{\v c}evi{\'c}, Djordje and Kirchknopf, Armin and Hecht, Manuel and Bogensperger, Johannes and Schlarb, Sven and Schindler, Alexander and Zeppelzauer, Matthias},
	month = feb,
	year = {2022},
	keywords = {sexism detection},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\FUJ8PTIV\\Sch{\"u}tz et al. - 2022 - Automatic Sexism Detection with Multilingual Trans.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\HQ6CJKJY\\2106.html:text/html},
}

@article{ashton_hexaco-60_2009,
	title = {The {HEXACO}-60: {A} {Short} {Measure} of the {Major} {Dimensions} of {Personality}},
	volume = {91},
	issn = {0022-3891},
	shorttitle = {The {HEXACO}-60},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00223890902935878},
	doi = {10.1080/00223890902935878},
	language = {en},
	number = {4},
	urldate = {2024-10-19},
	journal = {Journal of Personality Assessment},
	author = {Ashton, Michael and Lee, Kibeom},
	month = jul,
	year = {2009},
	pages = {340--345},
}

@article{schwartz_human_2015,
	title = {Human {Values} {Scale} ({ESS})},
	copyright = {Nutzungsbedingungen: Alle in ZIS dokumentierten Erhebungsinstrumente d{\"u}rfen kostenfrei f{\"u}r nicht-kommerzielle Forschungszwecke verwendet werden.Bei einem Einsatz f{\"u}r andere Zwecke oder in einer anderen als der hier dokumentierten Form ist das Einverst{\"a}ndnis der Autoren bzw. Autorinnen einzuholen. In allen resultierenden Arbeiten und Publikationen ist die ZIS-Dokumentation als Quelle anzugeben., Terms of use: All survey instruments documented in ZIS may be used free of charge for non-commercial research purposes. In the case of use for other purposes or in a form other than those documented here, the consent of the authors must be obtained. In all resulting works and publications, the ZIS documentation must be cited as the source.},
	url = {http://zis.gesis.org/DoiId/zis234},
	doi = {10.6102/ZIS234},
	abstract = {The Human Values Scale (HVS) of the
European Social Survey (ESS) is a well-established 21-item measure developed by
Shalom Schwartz. It classifies respondents according to ten basic value
orientations: achievement, benevolence, conformity, hedonism, power, security,
self-direction, stimulation, tradition, and universalism.},
	language = {en},
	urldate = {2024-10-19},
	journal = {ZIS - The Collection Items and Scales for the Social Sciences},
	author = {Schwartz, S. H. and Breyer, B. and Danner, D.},
	year = {2015},
	note = {Publisher: ZIS - GESIS Leibniz Institute for the Social Sciences
Version Number: 1.0},
}

@article{schudson_gendersex_2022,
	title = {Gender/sex diversity beliefs: {Scale} construction, validation, and links to prejudice},
	volume = {25},
	issn = {1368-4302},
	shorttitle = {Gender/sex diversity beliefs},
	url = {https://doi.org/10.1177/1368430220987595},
	doi = {10.1177/1368430220987595},
	abstract = {Prejudice against or affirmation of gender/sex minorities is often framed in terms of beliefs about the ontology of gender/sex (i.e., what gender/sex is), or gender/sex diversity beliefs. We constructed the Gender/Sex Diversity Beliefs Scale (GSDB) to assess ontological beliefs about the nature of gender/sex, including essentialist and social constructionist beliefs, and validated the GSDB across a series of studies. In Study 1 (N = 304), we explored the factor structure of the GSDB and found evidence of associations with prejudice against transgender and/or nonbinary people. In Study 2 (N = 300), we assessed the stability of the factor structure of the GSDB and examined its criterion-related validity, including its relationship to feelings toward multiple gender/sex groups. In Studies 3a (N = 48) and 3b (N = 500), we established test{\textendash}retest reliability. We conclude that gender/sex diversity beliefs are important for understanding contemporary attitudes about gender/sex, including prejudice against gender/sex minorities, and that the GSDB is a reliable and valid way to measure them.},
	number = {4},
	urldate = {2024-10-21},
	journal = {Group Processes \& Intergroup Relations},
	author = {Schudson, Zach C. and van Anders, Sari M.},
	month = jun,
	year = {2022},
	pages = {1011--1036},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\2SKPFSPH\\Schudson and van Anders - 2022 - Gendersex diversity beliefs Scale construction, .pdf:application/pdf},
}

@incollection{swim_sexism_2009,
	address = {New York, NY, USA},
	title = {Sexism},
	isbn = {978-0-8058-5952-2},
	abstract = {In this chapter we examine evidence about the prevalence of sexism by examining different ways in which sexist beliefs can be manifested, evidence documenting sexist behaviors, and some of the consequences of sexism. We define sexism as individuals' attitudes, beliefs, and behaviors, and organizational, institutional, and cultural practices that either reflect negative evaluations of individuals based on their gender or support unequal status of women and men. Most of the chapter focuses on an individual level of analysis and antifemale sexism, as these represent most of the psychological research on sexism. However, it is important to acknowledge all levels of analyses are intertwined and both women and men experience sexism. We begin by placing research on sexism within the historical context of the study of gender differences. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	booktitle = {Handbook of prejudice, stereotyping, and discrimination},
	publisher = {Psychology Press},
	author = {Swim, Janet K. and Hyers, Lauri L.},
	year = {2009},
	doi = {10.4324/9781841697772},
	pages = {407--430},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\EQB7IA8X\\2008-09974-020.html:text/html},
}

@incollection{swim_sexism_2003,
	title = {Sexism: {Attitudes}, {Beliefs}, and {Behaviors}},
	isbn = {978-0-470-69342-1},
	shorttitle = {Sexism},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470693421.ch11},
	abstract = {This chapter contains section titled: Attitudes toward Women Beliefs about Women Treatment of Women Comparisons with Other Forms of Prejudice and Discrimination Conclusions Notes References},
	language = {en},
	urldate = {2024-10-22},
	booktitle = {Blackwell {Handbook} of {Social} {Psychology}: {Intergroup} {Processes}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Swim, Janet K. and Campbell, Bernadette},
	year = {2003},
	doi = {10.1002/9780470693421.ch11},
	note = {Section: Eleven
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470693421.ch11},
	pages = {218--237},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\V7LAYMQU\\9780470693421.html:text/html;Swim and Campbell - 2003 - Sexism Attitudes, Beliefs, and Behaviors.pdf:C\:\\Users\\jana\\Zotero\\storage\\EB6YEQ29\\Swim and Campbell - 2003 - Sexism Attitudes, Beliefs, and Behaviors.pdf:application/pdf},
}

@article{swim_sexism_1995,
	title = {Sexism and racism: {Old}-fashioned and modern prejudices},
	volume = {68},
	issn = {1939-1315},
	shorttitle = {Sexism and racism},
	doi = {10.1037/0022-3514.68.2.199},
	abstract = {Prejudice and discrimination against women has become increasingly subtle and covert (N. V. Benokraitis \& J. R. Feagin, 1986). Unlike research on racism, little research about prejudice and discrimination against women has explicitly examined beliefs underlying this more modern form of sexism. Support was found for a distinction between old-fashioned and modern beliefs about women similar to results that have been presented for racism (J. B. McConahay, 1986; D. O. Sears, 1988). The former is characterized by endorsement of traditional gender roles, differential treatment of women and men, and stereotypes about lesser female competence. Like modern racism, modern sexism is characterized by the denial of continued discrimination, antagonism toward women's demands, and lack of support for policies designed to help women (for example, in education and work). Research that compares factor structures of old-fashioned and modern sexism and racism and that validates our modern sexism scale is presented. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {2},
	journal = {Journal of Personality and Social Psychology},
	author = {Swim, Janet K. and Aikin, Kathryn J. and Hall, Wayne S. and Hunter, Barbara A.},
	year = {1995},
	keywords = {MSS},
	pages = {199--214},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\4K6TYDAR\\1995-17261-001.html:text/html},
}

@article{mcconahay_modern_1980,
	title = {Modern racism scale},
	journal = {Personality and Social Psychology Bulletin},
	author = {McConahay, John B and Hardee, Betty B and Batts, Valerie},
	year = {1980},
	keywords = {MRS},
}

@article{cho_cronbachs_2015,
	title = {Cronbach{\textquoteright}s {Coefficient} {Alpha}: {Well} {Known} but {Poorly} {Understood}},
	volume = {18},
	issn = {1094-4281},
	shorttitle = {Cronbach{\textquoteright}s {Coefficient} {Alpha}},
	url = {https://doi.org/10.1177/1094428114555994},
	doi = {10.1177/1094428114555994},
	abstract = {This study disproves the following six common misconceptions about coefficient alpha: (a) Alpha was first developed by Cronbach. (b) Alpha equals reliability. (c) A high value of alpha is an indication of internal consistency. (d) Reliability will always be improved by deleting items using {\textquotedblleft}alpha if item deleted.{\textquotedblright} (e) Alpha should be greater than or equal to .7 (or, alternatively, .8). (f) Alpha is the best choice among all published reliability coefficients. This study discusses the inaccuracy of each of these misconceptions and provides a correct statement. This study recommends that the assumptions of unidimensionality and tau-equivalency be examined before the application of alpha and that structural equation modeling (SEM){\textendash}based reliability estimators be substituted for alpha when one of these conditions is not satisfied. This study also provides formulas for SEM-based reliability estimators that do not rely on matrix notation and step-by-step explanations for the computation of SEM-based reliability estimates.},
	language = {en},
	number = {2},
	urldate = {2024-10-24},
	journal = {Organizational Research Methods},
	author = {Cho, Eunseong and Kim, Seonghoon},
	month = apr,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {207--230},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\XK6JLWQC\\Cho and Kim - 2015 - Cronbach{\textquoteright}s Coefficient Alpha Well Known but Poorl.pdf:application/pdf},
}

@misc{frohling_personas_2024,
	title = {Personas with {Attitudes}: {Controlling} {LLMs} for {Diverse} {Data} {Annotation}},
	shorttitle = {Personas with {Attitudes}},
	url = {http://arxiv.org/abs/2410.11745},
	doi = {10.48550/arXiv.2410.11745},
	abstract = {We present a novel approach for enhancing diversity and control in data annotation tasks by personalizing large language models (LLMs). We investigate the impact of injecting diverse persona descriptions into LLM prompts across two studies, exploring whether personas increase annotation diversity and whether the impacts of individual personas on the resulting annotations are consistent and controllable. Our results show that persona-prompted LLMs produce more diverse annotations than LLMs prompted without personas and that these effects are both controllable and repeatable, making our approach a suitable tool for improving data annotation in subjective NLP tasks like toxicity detection.},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Fr{\"o}hling, Leon and Demartini, Gianluca and Assenmacher, Dennis},
	month = oct,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\YFSA5KNP\\Fr{\"o}hling et al. - 2024 - Personas with Attitudes Controlling LLMs for Dive.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\S34WSUM8\\2410.html:text/html},
}

@misc{tseng_two_2024,
	title = {Two {Tales} of {Persona} in {LLMs}: {A} {Survey} of {Role}-{Playing} and {Personalization}},
	shorttitle = {Two {Tales} of {Persona} in {LLMs}},
	url = {http://arxiv.org/abs/2406.01171},
	doi = {10.48550/arXiv.2406.01171},
	abstract = {The concept of persona, originally adopted in dialogue literature, has re-surged as a promising framework for tailoring large language models (LLMs) to specific context (e.g., personalized search, LLM-as-a-judge). However, the growing research on leveraging persona in LLMs is relatively disorganized and lacks a systematic taxonomy. To close the gap, we present a comprehensive survey to categorize the current state of the field. We identify two lines of research, namely (1) LLM Role-Playing, where personas are assigned to LLMs, and (2) LLM Personalization, where LLMs take care of user personas. Additionally, we introduce existing methods for LLM personality evaluation. To the best of our knowledge, we present the first survey for role-playing and personalization in LLMs under the unified view of persona. We continuously maintain a paper collection to foster future endeavors: https://github.com/MiuLab/PersonaLLM-Survey},
	urldate = {2024-10-30},
	publisher = {arXiv},
	author = {Tseng, Yu-Min and Huang, Yu-Chao and Hsiao, Teng-Yun and Chen, Wei-Lin and Huang, Chao-Wei and Meng, Yu and Chen, Yun-Nung},
	month = oct,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\DEVNUJEZ\\Tseng et al. - 2024 - Two Tales of Persona in LLMs A Survey of Role-Pla.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\WUP4MKFH\\2406.html:text/html},
}

@inproceedings{rottger_political_2024,
	address = {Bangkok, Thailand},
	title = {Political {Compass} or {Spinning} {Arrow}? {Towards} {More} {Meaningful} {Evaluations} for {Values} and {Opinions} in {Large} {Language} {Models}},
	shorttitle = {Political {Compass} or {Spinning} {Arrow}?},
	url = {https://aclanthology.org/2024.acl-long.816},
	doi = {10.18653/v1/2024.acl-long.816},
	abstract = {Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing *constrained* evaluation paradigm for values and opinions in LLMs and explore more realistic *unconstrained* evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT *forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.},
	urldate = {2024-10-30},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {R{\"o}ttger, Paul and Hofmann, Valentin and Pyatkin, Valentina and Hinck, Musashi and Kirk, Hannah and Schuetze, Hinrich and Hovy, Dirk},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {15295--15311},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\6NXNY6SD\\R{\"o}ttger et al. - 2024 - Political Compass or Spinning Arrow Towards More .pdf:application/pdf},
}

@article{schaffner_optimizing_2022,
	title = {Optimizing the {Measurement} of {Sexism} in {Political} {Surveys}},
	volume = {30},
	issn = {1047-1987, 1476-4989},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/optimizing-the-measurement-of-sexism-in-political-surveys/58A96CD10C45B2BFE66B585CAEB200F2},
	doi = {10.1017/pan.2021.6},
	abstract = {Political scientists are paying increasing attention to understanding the role of sexist attitudes on predicting vote choices and opinions on issues. However, the research in this area measures sexist attitudes with a variety of different items and scales. In this paper, I evaluate some of the most prominent contemporary measures of sexism and develop an approach for identifying optimal items based on (1) convergent validity, (2) predictive validity, and (3) distance from politics. I find that a subset of items from the hostile sexism scale exhibit the most desirable measurement properties and I conclude by recommending a simple two- to five-item reduced hostile sexism battery that will allow scholars to efficiently, validly, and consistently measure sexism.},
	language = {en},
	number = {3},
	urldate = {2024-11-06},
	journal = {Political Analysis},
	author = {Schaffner, Brian F.},
	month = jul,
	year = {2022},
	keywords = {predictive validity},
	pages = {364--380},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\LIDBTWGM\\Schaffner - 2022 - Optimizing the Measurement of Sexism in Political .pdf:application/pdf},
}

@article{archer_improving_2022,
	title = {Improving the {Measurement} of {Hostile} {Sexism}},
	volume = {86},
	issn = {0033-362X},
	url = {https://doi.org/10.1093/poq/nfac015},
	doi = {10.1093/poq/nfac015},
	abstract = {In recent years, sexism has played an increasingly pivotal role in American politics, and scholarship examining the importance of gender attitudes for political behavior has surged. Researchers have largely relied on the hostile sexism scale to measure prejudice against women, and this scale seems particularly relevant to political science research. However, this scale measures attitudes with an agree-disagree response format, which has long been recognized as a source of substantial measurement error. In this paper, we introduce a revised version of the hostile sexism scale that instead relies on an item-specific question format. Across three studies, we show that the item-specific scale is strongly related to the agree-disagree scale, but that the item-specific version reduces problems with truncation and tends to improve discriminant and predictive validity. Given these advantages, we conclude by recommending that researchers adopt the item-specific hostile sexism scale.},
	number = {2},
	urldate = {2024-11-06},
	journal = {Public Opinion Quarterly},
	author = {Archer, Allison M N and Clifford, Scott},
	month = jun,
	year = {2022},
	keywords = {predictive validity},
	pages = {223--246},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\2VML68QC\\Archer and Clifford - 2022 - Improving the Measurement of Hostile Sexism.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\WVTQWTNH\\6581516.html:text/html},
}

@article{mcconahay_self-interest_1982,
	title = {Self-{Interest} versus {Racial} {Attitudes} as {Correlates} of {Anti}-{Busing} {Attitudes} in {Louisville}: {Is} it {The} {Buses} or the {Blacks}?},
	volume = {44},
	issn = {0022-3816},
	shorttitle = {Self-{Interest} versus {Racial} {Attitudes} as {Correlates} of {Anti}-{Busing} {Attitudes} in {Louisville}},
	url = {https://www.journals.uchicago.edu/doi/abs/10.2307/2130514},
	doi = {10.2307/2130514},
	abstract = {Traditional political analysis assumes that policy attitudes of the American public are influenced more by personal self-interest than values or ideology. A symbolic politics approach holds that policy preferences are more the result of the residues of early political and value socialization, especially when the policies are linked to racial or ethnic groups. In this article, the relative strengths of the self-interest and symbolic politics approaches are tested in the context of the busing for school desegregation controversy in Louisville, Kentucky. Regression analysis of a random sample of Louisville adults found that measures of self-interest (having a child in the public schools, having a child bused, having strong ties to the neighborhood) were related only weakly and inconsistently to anti-busing attitudes whereas measures of racial attitudes (Old Fashioned and Modern Racism) were strong and consistent correlates of opposition to busing: the more prejudiced, the more opposed. The implications of these findings for school desegregation policy are discussed.},
	number = {3},
	urldate = {2024-11-12},
	journal = {The Journal of Politics},
	author = {McConahay, John B.},
	month = aug,
	year = {1982},
	note = {Publisher: The University of Chicago Press},
	keywords = {MRS},
	pages = {692--720},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\T9X8BBCZ\\McConahay - 1982 - Self-Interest versus Racial Attitudes as Correlate.pdf:application/pdf},
}

@article{glick_ambivalent_1996,
	title = {The ambivalent sexism inventory: {Differentiating} hostile and benevolent sexism},
	volume = {70},
	shorttitle = {The ambivalent sexism inventory},
	url = {https://www.taylorfrancis.com/chapters/edit/10.4324/9781315187280-6/ambivalent-sexism-inventory-peter-glick-susan-fiske},
	number = {3},
	urldate = {2024-11-13},
	journal = {Journal of Personality and Social Psychology},
	author = {Glick, Peter and Fiske, Susan T.},
	year = {1996},
	keywords = {ASI, predictive validity, nomological network},
	pages = {491--512},
	file = {Available Version (via Google Scholar):C\:\\Users\\jana\\Zotero\\storage\\GT5CFKDA\\Glick and Fiske - 2018 - The ambivalent sexism inventory Differentiating h.pdf:application/pdf},
}

@misc{wang_rolellm_2024,
	title = {{RoleLLM}: {Benchmarking}, {Eliciting}, and {Enhancing} {Role}-{Playing} {Abilities} of {Large} {Language} {Models}},
	shorttitle = {{RoleLLM}},
	url = {http://arxiv.org/abs/2310.00746},
	doi = {10.48550/arXiv.2310.00746},
	abstract = {The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for role-specific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4).},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Yang, Jian and Zhang, Man and Zhang, Zhaoxiang and Ouyang, Wanli and Xu, Ke and Huang, Stephen W. and Fu, Jie and Peng, Junran},
	month = jun,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\XHPC2JXR\\Wang et al. - 2024 - RoleLLM Benchmarking, Eliciting, and Enhancing Ro.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\8AHTXT7D\\2310.html:text/html},
}

@misc{hu_quantifying_2024,
	title = {Quantifying the {Persona} {Effect} in {LLM} {Simulations}},
	url = {http://arxiv.org/abs/2402.10811},
	doi = {10.48550/arXiv.2402.10811},
	abstract = {Large language models (LLMs) have shown remarkable promise in simulating human language and behavior. This study investigates how integrating persona variables-demographic, social, and behavioral factors-impacts LLMs' ability to simulate diverse perspectives. We find that persona variables account for {\textless}10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating persona variables via prompting in LLMs provides modest but statistically significant improvements. Persona prompting is most effective in samples where many annotators disagree, but their disagreements are relatively minor. Notably, we find a linear relationship in our setting: the stronger the correlation between persona variables and human annotations, the more accurate the LLM predictions are using persona prompting. In a zero-shot setting, a powerful 70b model with persona prompting captures 81\% of the annotation variance achievable by linear regression trained on ground truth annotations. However, for most subjective NLP datasets, where persona variables have limited explanatory power, the benefits of persona prompting are limited.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Hu, Tiancheng and Collier, Nigel},
	month = jun,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\NKNR3QFM\\Hu and Collier - 2024 - Quantifying the Persona Effect in LLM Simulations.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\8FKPLJCS\\2402.html:text/html},
}

@misc{ge_scaling_2024,
	title = {Scaling {Synthetic} {Data} {Creation} with 1,000,000,000 {Personas}},
	url = {http://arxiv.org/abs/2406.20094},
	doi = {10.48550/arXiv.2406.20094},
	abstract = {We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas ({\textasciitilde}13\% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Ge, Tao and Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
	month = sep,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\8ZLXQYV8\\Ge et al. - 2024 - Scaling Synthetic Data Creation with 1,000,000,000.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\CPQEISYR\\2406.html:text/html},
}

@misc{gupta_bias_2024,
	title = {Bias {Runs} {Deep}: {Implicit} {Reasoning} {Biases} in {Persona}-{Assigned} {LLMs}},
	shorttitle = {Bias {Runs} {Deep}},
	url = {http://arxiv.org/abs/2311.04892},
	doi = {10.48550/arXiv.2311.04892},
	abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
	month = jan,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\D5AJH2VY\\Gupta et al. - 2024 - Bias Runs Deep Implicit Reasoning Biases in Perso.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\N5QPPVXU\\2311.html:text/html},
}

@article{nazir_comprehensive_2023,
	title = {A comprehensive survey of {ChatGPT}: {Advancements}, applications, prospects, and challenges},
	volume = {1},
	issn = {2950-1628},
	shorttitle = {A comprehensive survey of {ChatGPT}},
	url = {https://www.sciencedirect.com/science/article/pii/S295016282300022X},
	doi = {10.1016/j.metrad.2023.100022},
	abstract = {Large Language Models (LLMs) especially when combined with Generative Pre-trained Transformers (GPT) represent a groundbreaking in natural language processing. In particular, ChatGPT, a state-of-the-art conversational language model with a user-friendly interface, has garnered substantial attention owing to its remarkable capability for generating human-like responses across a variety of conversational scenarios. This survey offers an overview of ChatGPT, delving into its inception, evolution, and key technology. We summarize the fundamental principles that underpin ChatGPT, encompassing its introduction in conjunction with GPT and LLMs. We also highlight the specific characteristics of GPT models with details of their impressive language understanding and generation capabilities. We then summarize applications of ChatGPT in a few representative domains. In parallel to the many advantages that ChatGPT can provide, we discuss the limitations and challenges along with potential mitigation strategies. Despite various controversial arguments and ethical concerns, ChatGPT has drawn significant attention from research industries and academia in a very short period. The survey concludes with an envision of promising avenues for future research in the field of ChatGPT. It is worth noting that knowing and addressing the challenges faced by ChatGPT will mount the way for more reliable and trustworthy conversational agents in the years to come.},
	number = {2},
	urldate = {2024-11-26},
	journal = {Meta-Radiology},
	author = {Nazir, Anam and Wang, Ze},
	month = sep,
	year = {2023},
	pages = {100022},
	file = {Accepted Version:C\:\\Users\\jana\\Zotero\\storage\\8JW8AFDP\\Nazir and Wang - 2023 - A comprehensive survey of ChatGPT Advancements, a.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\7EY7H435\\S295016282300022X.html:text/html},
}

@misc{adilazuarda_towards_2024,
	title = {Towards {Measuring} and {Modeling} "{Culture}" in {LLMs}: {A} {Survey}},
	shorttitle = {Towards {Measuring} and {Modeling} "{Culture}" in {LLMs}},
	url = {http://arxiv.org/abs/2403.15412},
	doi = {10.48550/arXiv.2403.15412},
	abstract = {We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define "culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Adilazuarda, Muhammad Farid and Mukherjee, Sagnik and Lavania, Pradhyumna and Singh, Siddhant and Aji, Alham Fikri and O'Neill, Jacki and Modi, Ashutosh and Choudhury, Monojit},
	month = sep,
	year = {2024},
	note = {arXiv:2403.15412},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\LFQ6MHDJ\\Adilazuarda et al. - 2024 - Towards Measuring and Modeling Culture in LLMs .pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\79H6J3JJ\\2403.html:text/html},
}

@inproceedings{ouyang_shifted_2023,
	address = {Singapore},
	title = {The {Shifted} and {The} {Overlooked}: {A} {Task}-oriented {Investigation} of {User}-{GPT} {Interactions}},
	shorttitle = {The {Shifted} and {The} {Overlooked}},
	url = {https://aclanthology.org/2023.emnlp-main.146},
	doi = {10.18653/v1/2023.emnlp-main.146},
	abstract = {Recent progress in Large Language Models (LLMs) has produced models that exhibit remarkable performance across a variety of NLP tasks. However, it remains unclear whether the existing focus of NLP research accurately captures the genuine requirements of human users. This paper provides a comprehensive analysis of the divergence between academic research in NLP and the needs of real-world NLP applications via a large-scale collection of user-GPT conversations. We analyze a large-scale collection of real user queries to GPT. We compare these queries against existing NLP benchmark tasks and identify a significant gap between the tasks that users frequently request from LLMs and the tasks that are commonly studied in academic research. For example, we find that tasks such as {\textquotedblleft}design{\textquotedblright} and {\textquotedblleft}planning{\textquotedblright} are prevalent in user interactions but largely neglected or different from traditional NLP benchmarks. We investigate these overlooked tasks, dissect the practical challenges, and provide insights toward a roadmap to make LLMs better aligned with user needs.},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ouyang, Siru and Wang, Shuohang and Liu, Yang and Zhong, Ming and Jiao, Yizhu and Iter, Dan and Pryzant, Reid and Zhu, Chenguang and Ji, Heng and Han, Jiawei},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {2375--2393},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\LX8GKTYG\\Ouyang et al. - 2023 - The Shifted and The Overlooked A Task-oriented In.pdf:application/pdf},
}

@inproceedings{huang_humanity_2023,
	title = {On the {Humanity} of {Conversational} {AI}: {Evaluating} the {Psychological} {Portrayal} of {LLMs}},
	shorttitle = {On the {Humanity} of {Conversational} {AI}},
	url = {https://openreview.net/forum?id=H3UayAQWoE},
	abstract = {Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.},
	urldate = {2024-11-27},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Huang, Jen-tse and Wang, Wenxuan and Li, Eric John and Lam, Man Ho and Ren, Shujie and Yuan, Youliang and Jiao, Wenxiang and Tu, Zhaopeng and Lyu, Michael},
	month = oct,
	year = {2023},
	keywords = {machine psychology, individual},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\P6EMIKM4\\Huang et al. - 2023 - On the Humanity of Conversational AI Evaluating t.pdf:application/pdf},
}

@misc{zhao_bias_2024,
	title = {Bias and {Toxicity} in {Role}-{Play} {Reasoning}},
	url = {http://arxiv.org/abs/2409.13979},
	doi = {10.48550/arXiv.2409.13979},
	abstract = {Role-play in the Large Language Model (LLM) is a crucial technique that enables models to adopt specific perspectives, enhancing their ability to generate contextually relevant and accurate responses. By simulating different roles, theis approach improves reasoning capabilities across various NLP benchmarks, making the model's output more aligned with diverse scenarios. However, in this work, we demonstrate that role-play also carries potential risks. We systematically evaluate the impact of role-play by asking the language model to adopt different roles and testing it on multiple benchmarks that contain stereotypical and harmful questions. Despite the significant fluctuations in the benchmark results in different experiments, we find that applying role-play often increases the overall likelihood of generating stereotypical and harmful outputs.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Zhao, Jinman and Qian, Zifan and Cao, Linbo and Wang, Yining and Ding, Yitian},
	month = sep,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\66X4FLK7\\Zhao et al. - 2024 - Bias and Toxicity in Role-Play Reasoning.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\Z99SIPZQ\\2409.html:text/html},
}

@misc{zhang_hire_2024,
	title = {Hire {Me} or {Not}? {Examining} {Language} {Model}'s {Behavior} with {Occupation} {Attributes}},
	shorttitle = {Hire {Me} or {Not}?},
	url = {http://arxiv.org/abs/2405.06687},
	doi = {10.48550/arXiv.2405.06687},
	abstract = {With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.},
	urldate = {2024-12-17},
	publisher = {arXiv},
	author = {Zhang, Damin and Zhang, Yi and Bihani, Geetanjali and Rayz, Julia},
	month = nov,
	year = {2024},
	keywords = {occupational bias, predictive validity},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\AZ2RER28\\Zhang et al. - 2024 - Hire Me or Not Examining Language Model's Behavio.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\XKVT8XY5\\2405.html:text/html},
}

@article{trizano-hermosilla_best_2016,
	title = {Best {Alternatives} to {Cronbach}'s {Alpha} {Reliability} in {Realistic} {Conditions}: {Congeneric} and {Asymmetrical} {Measurements}},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Best {Alternatives} to {Cronbach}'s {Alpha} {Reliability} in {Realistic} {Conditions}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2016.00769/full},
	doi = {10.3389/fpsyg.2016.00769},
	abstract = {{\textless}p{\textgreater}The Cronbach's alpha is the most widely used method for estimating internal consistency reliability. This procedure has proved very resistant to the passage of time, even if its limitations are well documented and although there are better options as omega coefficient or the different versions of glb, with obvious advantages especially for applied research in which the {\'i}tems differ in quality or have skewed distributions. In this paper, using Monte Carlo simulation, the performance of these reliability coefficients under a one-dimensional model is evaluated in terms of skewness and no tau-equivalence. The results show that omega coefficient is always better choice than alpha and in the presence of skew items is preferable to use omega and glb coefficients even in small samples.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-03},
	journal = {Frontiers in Psychology},
	author = {Trizano-Hermosilla, Italo and Alvarado, Jes{\'u}s M.},
	month = may,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {reliability},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\XQR4SDN7\\Trizano-Hermosilla and Alvarado - 2016 - Best Alternatives to Cronbach's Alpha Reliability .pdf:application/pdf},
}

@article{cho_making_2016,
	title = {Making {Reliability} {Reliable}: {A} {Systematic} {Approach} to {Reliability} {Coefficients}},
	volume = {19},
	issn = {1094-4281},
	shorttitle = {Making {Reliability} {Reliable}},
	url = {https://doi.org/10.1177/1094428116656239},
	doi = {10.1177/1094428116656239},
	abstract = {The current conventions for test score reliability coefficients are unsystematic and chaotic. Reliability coefficients have long been denoted using names that are unrelated to each other, with each formula being generated through different methods, and they have been represented inconsistently. Such inconsistency prevents organizational researchers from understanding the whole picture and misleads them into using coefficient alpha unconditionally. This study provides a systematic naming convention, formula-generating methods, and methods of representing each of the reliability coefficients. This study offers an easy-to-use solution to the issue of choosing between coefficient alpha and composite reliability. This study introduces a calculator that enables its users to obtain the values of various multidimensional reliability coefficients with a few mouse clicks. This study also presents illustrative numerical examples to provide a better understanding of the characteristics and computations of reliability coefficients.},
	number = {4},
	urldate = {2025-01-03},
	journal = {Organizational Research Methods},
	author = {Cho, Eunseong},
	month = oct,
	year = {2016},
	keywords = {reliability},
	pages = {651--682},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\G9SLI9FN\\Cho - 2016 - Making Reliability Reliable A Systematic Approach.pdf:application/pdf},
}

@article{mcneish_thanks_2018,
	title = {Thanks coefficient alpha, we{\textquoteright}ll take it from here.},
	volume = {23},
	issn = {1939-1463, 1082-989X},
	url = {https://doi.apa.org/doi/10.1037/met0000144},
	doi = {10.1037/met0000144},
	language = {en},
	number = {3},
	urldate = {2025-01-03},
	journal = {Psychological Methods},
	author = {McNeish, Daniel},
	month = sep,
	year = {2018},
	keywords = {reliability},
	pages = {412--433},
	file = {McNeish - 2018 - Thanks coefficient alpha, we{\textquoteright}ll take it from here..pdf:C\:\\Users\\jana\\Zotero\\storage\\46TZVUTI\\McNeish - 2018 - Thanks coefficient alpha, we{\textquoteright}ll take it from here..pdf:application/pdf},
}

@book{field_discovering_2012,
	address = {Los Angeles, California USA},
	title = {Discovering statistics using {R}},
	isbn = {978-1-4462-8913-6},
	publisher = {Sage},
	author = {Field, Andy and Miles, Jeremy and Field, Zo{\"e}},
	year = {2012},
	file = {Field_ea_2012_Discovering Statistics using R.pdf:C\:\\Users\\jana\\Zotero\\storage\\LJJDIH67\\Field_ea_2012_Discovering Statistics using R.pdf:application/pdf},
}

@book{moosbrugger_testtheorie_2012,
	address = {Berlin, Germany},
	edition = {2nd},
	title = {Testtheorie und {Fragebogenkonstruktion}},
	isbn = {978-3-642-20071-7},
	publisher = {Springer},
	author = {Moosbrugger, Helfried and Kelava, Augustin},
	year = {2012},
	keywords = {reliability, validity, item statistics},
}

@misc{varadarajan_consistent_2025,
	title = {The {Consistent} {Lack} of {Variance} of {Psychological} {Factors} {Expressed} by {LLMs} and {Spambots}},
	url = {https://sjgiorgi.github.io/publications/varadarajan2025consistent.pdf},
	urldate = {2025-01-13},
	author = {Varadarajan, Vasudha and Giorgi, Salvatore and Mangalik, Siddharth and Soni, Nikita and Markowitz, David M. and Schwartz, H. Andrew},
	year = {2025},
	keywords = {important},
	file = {Available Version (via Google Scholar):C\:\\Users\\jana\\Zotero\\storage\\LCC6M9VE\\Varadarajan et al. - The Consistent Lack of Variance of Psychological F.pdf:application/pdf},
}

@incollection{rammstedt_reliabilitat_2010,
	address = {Wiesbaden, Germany},
	title = {Reliabilit{\"a}t, {Validit{\"a}t}, {Objektivit{\"a}t}},
	isbn = {978-3-531-92038-2},
	url = {https://doi.org/10.1007/978-3-531-92038-2_11},
	abstract = {Das folgende Kapitel beschreibt den Weg von dem zu messenden Merkmal {\"u}ber die Erstellung eines Erhebungsinstruments bis zum Messwert. Schwerpunkt des Kapitels liegt auf der G{\"u}tebestimmung f{\"u}r diese Erhebungsinstrumente, n{\"a}mlich auf der {\"U}berpr{\"u}fung der Objektivit{\"a}t, Reliabilit{\"a}t und Validit{\"a}t von Messverfahren. Nur die {\"U}berpr{\"u}fung und damit die Gew{\"a}hrleistung der G{\"u}te eines Erhebungsverfahrens rechtfertigen dessen Einsatz sowie die aus der Untersuchung gezogenen Schl{\"u}sse. So kann mittels eines nicht reliablen Verfahrens zur Bestimmung der Lesekompetenz nicht bestimmt werden, ob eine Gruppe von Befragten kompetenter ist als eine andere, da die erhaltenen Testergebnisse zu hohem Ma{\ss}e von Messfehlern beeinflusst sind und somit nicht als valide erachtet werden k{\"o}nnen. F{\"u}r die einzelnen G{\"u}temerkmale werden Definitionen, wo angebracht mathematische Grundlagen und Darstellungen zur Berechnung berichtet und deren Anwendung an einem Beispiel verdeutlicht.},
	urldate = {2025-01-14},
	booktitle = {Handbuch der sozialwissenschaftlichen {Datenanalyse}},
	publisher = {VS Verlag f{\"u}r Sozialwissenschaften},
	author = {Rammstedt, Beatrice},
	editor = {Wolf, Christof and Best, Henning},
	year = {2010},
	keywords = {reliability, validity},
	pages = {239--258},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\QQMAWJY2\\Rammstedt - 2010 - Reliabilit{\"a}t, Validit{\"a}t, Objektivit{\"a}t.pdf:application/pdf},
}

@article{revelle_reliability_2019,
	title = {Reliability from $\alpha$ to $\omega$: {A} tutorial},
	volume = {31},
	issn = {1939-134X},
	shorttitle = {Reliability from $\alpha$ to $\omega$},
	doi = {10.1037/pas0000754},
	abstract = {Reliability is a fundamental problem for measurement in all of science. Although defined in multiple ways, and estimated in even more ways, the basic concepts seem straightforward and need to be understood by practitioners as well as methodologists. Reliability theory is not just for the psychometrician estimating latent variables, it is for everyone who wants to make inferences from measures of individuals or of groups. For the case of a single test administration, we consider multiple measures of reliability, ranging from the worst ($\beta$) to average ($\alpha$, $\lambda$3) to best ($\lambda$4) split half reliabilities, and consider why model-based estimates ($\omega$h, $\omega$t) should be reported. We also address the utility of test-retest and alternate form reliabilities. The advantages of immediate versus delayed retests to decompose observed score variance into specific, state, and trait scores are discussed. But reliability is not just for test scores, it is also important when evaluating the use of ratings. Estimates that may be applied to continuous data include a set of intraclass correlations while discrete categorical data needs to take advantage of the family of $\kappa$ statistics. Examples of these various reliability estimates are given using state and trait measures of anxiety given with different delays and under different conditions. An online supplemental materials is provided with more detail and elaboration. The online supplemental materials is also used to demonstrate applications of open source software to examples of real data, and comparisons are made between the many types of reliability. (PsycINFO Database Record (c) 2019 APA, all rights reserved).},
	number = {12},
	journal = {Psychological Assessment},
	author = {Revelle, William and Condon, David M.},
	month = dec,
	year = {2019},
	keywords = {reliability, important},
	pages = {1395--1411},
	file = {Revelle and Condon - 2019 - Reliability from $\alpha$ to $\omega$ A tutorial.pdf:C\:\\Users\\jana\\Zotero\\storage\\UCTECG8L\\Revelle and Condon - 2019 - Reliability from $\alpha$ to $\omega$ A tutorial.pdf:application/pdf},
}

@inproceedings{gupta_self-assessment_2024,
	address = {Miami, Florida, USA},
	title = {Self-{Assessment} {Tests} are {Unreliable} {Measures} of {LLM} {Personality}},
	url = {https://aclanthology.org/2024.blackboxnlp-1.20/},
	doi = {10.18653/v1/2024.blackboxnlp-1.20},
	abstract = {As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of {\textquotedblleft}personality{\textquotedblright} of LLMs using self-assessment personality tests developed to measure human personality. Yet almost none of these works verify the applicability of these tests on LLMs. In this paper, we analyze the reliability of LLM personality scores obtained from self-assessment personality tests using two simple experiments. We first introduce the property of prompt sensitivity, where three semantically equivalent prompts representing three intuitive ways of administering self-assessment tests on LLMs are used to measure the personality of the same LLM. We find that all three prompts lead to very different personality scores, a difference that is statistically significant for all traits in a large majority of scenarios. We then introduce the property of option-order symmetry for personality measurement of LLMs. Since most of the self-assessment tests exist in the form of multiple choice question (MCQ) questions, we argue that the scores should also be robust to not just the prompt template but also the order in which the options are presented. This test unsurprisingly reveals that the self-assessment test scores are not robust to the order of the options. These simple tests, done on ChatGPT and three Llama2 models of different sizes, show that self-assessment personality tests created for humans are unreliable measures of personality in LLMs.},
	urldate = {2025-01-24},
	booktitle = {Proceedings of the 7th {BlackboxNLP} {Workshop}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Akshat and Song, Xiaoyang and Anumanchipalli, Gopala},
	editor = {Belinkov, Yonatan and Kim, Najoung and Jumelet, Jaap and Mohebbi, Hosein and Mueller, Aaron and Chen, Hanjie},
	month = nov,
	year = {2024},
	keywords = {criticism, option order symmetry},
	pages = {301--314},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\B4XDEVIU\\Gupta et al. - 2024 - Self-Assessment Tests are Unreliable Measures of L.pdf:application/pdf},
}

@misc{song_have_2023,
	title = {Have {Large} {Language} {Models} {Developed} a {Personality}?: {Applicability} of {Self}-{Assessment} {Tests} in {Measuring} {Personality} in {LLMs}},
	shorttitle = {Have {Large} {Language} {Models} {Developed} a {Personality}?},
	url = {http://arxiv.org/abs/2305.14693},
	doi = {10.48550/arXiv.2305.14693},
	abstract = {Have Large Language Models (LLMs) developed a personality? The short answer is a resounding "We Don't Know!". In this paper, we show that we do not yet have the right tools to measure personality in language models. Personality is an important characteristic that influences behavior. As LLMs emulate human-like intelligence and performance in various tasks, a natural question to ask is whether these models have developed a personality. Previous works have evaluated machine personality through self-assessment personality tests, which are a set of multiple-choice questions created to evaluate personality in humans. A fundamental assumption here is that human personality tests can accurately measure personality in machines. In this paper, we investigate the emergence of personality in five LLMs of different sizes ranging from 1.5B to 30B. We propose the Option-Order Symmetry property as a necessary condition for the reliability of these self-assessment tests. Under this condition, the answer to self-assessment questions is invariant to the order in which the options are presented. We find that many LLMs personality test responses do not preserve option-order symmetry. We take a deeper look at LLMs test responses where option-order symmetry is preserved to find that in these cases, LLMs do not take into account the situational statement being tested and produce the exact same answer irrespective of the situation being tested. We also identify the existence of inherent biases in these LLMs which is the root cause of the aforementioned phenomenon and makes self-assessment tests unreliable. These observations indicate that self-assessment tests are not the correct tools to measure personality in LLMs. Through this paper, we hope to draw attention to the shortcomings of current literature in measuring personality in LLMs and call for developing tools for machine personality measurement.},
	urldate = {2025-01-24},
	publisher = {arXiv},
	author = {Song, Xiaoyang and Gupta, Akshat and Mohebbizadeh, Kiyan and Hu, Shujie and Singh, Anant},
	month = may,
	year = {2023},
	keywords = {criticism, option order symmetry},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\6ZKAVEFJ\\Song et al. - 2023 - Have Large Language Models Developed a Personality.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\QDV4WXEC\\2305.html:text/html},
}

@inproceedings{sun_building_2024,
	address = {New York, NY, USA},
	series = {{CUI} '24},
	title = {Building {Better} {AI} {Agents}: {A} {Provocation} on the {Utilisation} of {Persona} in {LLM}-based {Conversational} {Agents}},
	isbn = {9798400705113},
	shorttitle = {Building {Better} {AI} {Agents}},
	url = {https://dl.acm.org/doi/10.1145/3640794.3665887},
	doi = {10.1145/3640794.3665887},
	abstract = {The incorporation of Large Language Models (LLMs) such as the GPT series into diverse sectors including healthcare, education, and finance marks a significant evolution in the field of artificial intelligence (AI). The increasing demand for personalised applications motivated the design of conversational agents (CAs) to possess distinct personas. This paper commences by examining the rationale and implications of imbuing CAs with unique personas, smoothly transitioning into a broader discussion of the personalisation and anthropomorphism of CAs based on LLMs in the LLM era. We delve into the specific applications where the implementation of a persona is not just beneficial but critical for LLM-based CAs. The paper underscores the necessity of a nuanced approach to persona integration, highlighting the potential challenges and ethical dilemmas that may arise. Attention is directed towards the importance of maintaining persona consistency, establishing robust evaluation mechanisms, and ensuring that the persona attributes are effectively complemented by domain-specific knowledge.},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 6th {ACM} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Sun, Guangzhi and Zhan, Xiao and Such, Jose},
	month = jul,
	year = {2024},
	keywords = {consistency},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\2W86KRE4\\Sun et al. - 2024 - Building Better AI Agents A Provocation on the Ut.pdf:application/pdf},
}

@inproceedings{shu_you_2024,
	address = {Mexico City, Mexico},
	title = {You don`t need a personality test to know these models are unreliable: {Assessing} the {Reliability} of {Large} {Language} {Models} on {Psychometric} {Instruments}},
	shorttitle = {You don`t need a personality test to know these models are unreliable},
	url = {https://aclanthology.org/2024.naacl-long.295/},
	doi = {10.18653/v1/2024.naacl-long.295},
	abstract = {The versatility of Large Language Models (LLMs) on natural language understanding tasks has made them popular for research in social sciences. To properly understand the properties and innate personas of LLMs, researchers have performed studies that involve using prompts in the form of questions that ask LLMs about particular opinions. In this study, we take a cautionary step back and examine whether the current format of prompting LLMs elicits responses in a consistent and robust manner. We first construct a dataset that contains 693 questions encompassing 39 different instruments of persona measurement on 115 persona axes. Additionally, we design a set of prompts containing minor variations and examine LLMs' capabilities to generate answers, as well as prompt variations to examine their consistency with respect to content-level variations such as switching the order of response options or negating the statement. Our experiments on 17 different LLMs reveal that even simple perturbations significantly downgrade a model`s question-answering ability, and that most LLMs have low negation consistency. Our results suggest that the currently widespread practice of prompting is insufficient to accurately and reliably capture model perceptions, and we therefore discuss potential alternatives to improve these issues.},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 2024 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shu, Bangzhao and Zhang, Lechen and Choi, Minje and Dunagan, Lavinia and Logeswaran, Lajanugen and Lee, Moontae and Card, Dallas and Jurgens, David},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	keywords = {criticism},
	pages = {5263--5281},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\8ZXVJNGX\\Shu et al. - 2024 - You don`t need a personality test to know these mo.pdf:application/pdf},
}

@inproceedings{garcia-ferrero_this_2023,
	address = {Singapore},
	title = {This is not a {Dataset}: {A} {Large} {Negation} {Benchmark} to {Challenge} {Large} {Language} {Models}},
	shorttitle = {This is not a {Dataset}},
	url = {https://aclanthology.org/2023.emnlp-main.531/},
	doi = {10.18653/v1/2023.emnlp-main.531},
	abstract = {Although large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available.},
	urldate = {2025-01-27},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Garc{\'i}a-Ferrero, Iker and Altuna, Bego{\~n}a and Alvez, Javier and Gonzalez-Dios, Itziar and Rigau, German},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {8596--8615},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\9UFAMVFF\\Garc{\'i}a-Ferrero et al. - 2023 - This is not a Dataset A Large Negation Benchmark .pdf:application/pdf},
}

@misc{petrov_limited_2024,
	title = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}: a {Psychometric} {Analysis}},
	shorttitle = {Limited {Ability} of {LLMs} to {Simulate} {Human} {Psychological} {Behaviours}},
	url = {http://arxiv.org/abs/2405.07248},
	doi = {10.48550/arXiv.2405.07248},
	abstract = {The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.},
	urldate = {2025-01-28},
	publisher = {arXiv},
	author = {Petrov, Nikolay B. and Serapio-Garc{\'i}a, Gregory and Rentfrow, Jason},
	month = may,
	year = {2024},
	keywords = {silicon sampling},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\H7M3G2FM\\Petrov et al. - 2024 - Limited Ability of LLMs to Simulate Human Psycholo.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\Z2R49NFI\\2405.html:text/html},
}

@article{park_diminished_2024,
	title = {Diminished diversity-of-thought in a standard large language model},
	volume = {56},
	issn = {1554-3528},
	url = {https://doi.org/10.3758/s13428-023-02307-x},
	doi = {10.3758/s13428-023-02307-x},
	abstract = {We test whether large language models (LLMs) can be used to simulate human participants in social-science studies. To do this, we ran replications of 14 studies from the Many Labs 2 replication project with OpenAI{\textquoteright}s text-davinci-003 model, colloquially known as GPT-3.5. Based on our pre-registered analyses, we find that among the eight studies we could analyse, our GPT sample replicated 37.5\% of the original results and 37.5\% of the Many Labs 2 results. However, we were unable to analyse the remaining six studies due to an unexpected phenomenon we call the {\textquotedblleft}correct answer{\textquotedblright} effect. Different runs of GPT-3.5 answered nuanced questions probing political orientation, economic preference, judgement, and moral philosophy with zero or near-zero variation in responses: with the supposedly {\textquotedblleft}correct answer.{\textquotedblright} In one exploratory follow-up study, we found that a {\textquotedblleft}correct answer{\textquotedblright} was robust to changing the demographic details that precede the prompt. In another, we found that most but not all {\textquotedblleft}correct answers{\textquotedblright} were robust to changing the order of answer choices. One of our most striking findings occurred in our replication of the Moral Foundations Theory survey results, where we found GPT-3.5 identifying as a political conservative in 99.6\% of the cases, and as a liberal in 99.3\% of the cases in the reverse-order condition. However, both self-reported {\textquoteleft}GPT conservatives{\textquoteright} and {\textquoteleft}GPT liberals{\textquoteright} showed right-leaning moral foundations. Our results cast doubts on the validity of using LLMs as a general replacement for human participants in the social sciences. Our results also raise concerns that a hypothetical AI-led future may be subject to a diminished diversity of thought.},
	number = {6},
	urldate = {2025-01-28},
	journal = {Behavior Research Methods},
	author = {Park, Peter S. and Schoenegger, Philipp and Zhu, Chongyang},
	month = sep,
	year = {2024},
	keywords = {silicon sampling},
	pages = {5754--5770},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\N4PVJWKI\\Park et al. - 2024 - Diminished diversity-of-thought in a standard larg.pdf:application/pdf},
}

@misc{bernardelle_mapping_2024,
	title = {Mapping and {Influencing} the {Political} {Ideology} of {Large} {Language} {Models} using {Synthetic} {Personas}},
	url = {http://arxiv.org/abs/2412.14843},
	doi = {10.48550/arXiv.2412.14843},
	abstract = {The analysis of political biases in large language models (LLMs) has primarily examined these systems as single entities with fixed viewpoints. While various methods exist for measuring such biases, the impact of persona-based prompting on LLMs' political orientation remains unexplored. In this work we leverage PersonaHub, a collection of synthetic persona descriptions, to map the political distribution of persona-based prompted LLMs using the Political Compass Test (PCT). We then examine whether these initial compass distributions can be manipulated through explicit ideological prompting towards diametrically opposed political orientations: right-authoritarian and left-libertarian. Our experiments reveal that synthetic personas predominantly cluster in the left-libertarian quadrant, with models demonstrating varying degrees of responsiveness when prompted with explicit ideological descriptors. While all models demonstrate significant shifts towards right-authoritarian positions, they exhibit more limited shifts towards left-libertarian positions, suggesting an asymmetric response to ideological manipulation that may reflect inherent biases in model training.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Bernardelle, Pietro and Fr{\"o}hling, Leon and Civelli, Stefano and Lunardi, Riccardo and Roitero, Kevin and Demartini, Gianluca},
	month = dec,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\EYQ4G3CQ\\Bernardelle et al. - 2024 - Mapping and Influencing the Political Ideology of .pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\7KEK39RE\\2412.html:text/html},
}

@book{moosbrugger_testtheorie_2020,
	address = {Berlin, Germany},
	edition = {3rd},
	title = {Testtheorie und {Fragebogenkonstruktion}},
	isbn = {978-3-662-61531-7},
	publisher = {Springer},
	editor = {Moosbrugger, Helfried and Kelava, Augustin},
	year = {2020},
	file = {Moosbrugger and Kelava - 2020 - Testtheorie und Fragebogenkonstruktion.pdf:C\:\\Users\\jana\\Zotero\\storage\\2J8QYIFK\\Moosbrugger and Kelava - 2020 - Testtheorie und Fragebogenkonstruktion.pdf:application/pdf},
}

@misc{arora_probing_2023,
	title = {Probing {Pre}-{Trained} {Language} {Models} for {Cross}-{Cultural} {Differences} in {Values}},
	url = {http://arxiv.org/abs/2203.13722},
	doi = {10.48550/arXiv.2203.13722},
	abstract = {Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.},
	urldate = {2023-10-11},
	publisher = {arXiv},
	author = {Arora, Arnav and Kaffee, Lucie-Aim{\'e}e and Augenstein, Isabelle},
	month = apr,
	year = {2023},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\U4LS8WVU\\Arora et al. - 2023 - Probing Pre-Trained Language Models for Cross-Cult.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\P276L8CP\\2203.html:text/html},
}

@article{madera_gender_2009,
	title = {Gender and letters of recommendation for academia: {Agentic} and communal differences.},
	volume = {94},
	issn = {0021-9010},
	shorttitle = {Gender and letters of recommendation for academia},
	url = {https://research.ebsco.com/linkprocessor/plink?id=e84ad8dc-7201-3dad-b51b-fe8974de335a},
	doi = {10.1037/a0016539},
	abstract = {In 2 studies that draw from the social role theory of sex differences (A. H. Eagly, W. Wood, \&amp; A. B. Diekman, 2000), the authors investigated differences in agentic and communal characteristics in letters of recommendation for men and women for academic positions and whether such differences influenced selection decisions in academia. The results supported the hypotheses, indicating (a) that women were described as more communal and less agentic than men (Study 1) and (b) that communal characteristics have a negative relationship with hiring decisions in academia that are based on letters of recommendation (Study 2). Such results are particularly important because letters of recommendation continue to be heavily weighted and commonly used selection tools (R. D. Arvey \&amp; T. E. Campion, 1982; R. M. Guion, 1998), particularly in academia (E. P. Sheehan, T. M. McDevitt, \&amp; H. C. Ross, 1998). (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {6},
	urldate = {2025-02-06},
	journal = {Journal of Applied Psychology},
	author = {Madera, Juan M. and Hebl, Michelle R. and Martin, Randi C.},
	month = nov,
	year = {2009},
	pages = {1591--1599},
	file = {Full text PDF:C\:\\Users\\jana\\Zotero\\storage\\XZV5HMPW\\Madera et al. - 2009 - Gender and letters of recommendation for academia.pdf:application/pdf},
}

@article{khan_gender_2023,
	title = {Gender bias in reference letters for residency and academic medicine: a systematic review},
	volume = {99},
	issn = {0032-5473},
	shorttitle = {Gender bias in reference letters for residency and academic medicine},
	url = {https://doi.org/10.1136/postgradmedj-2021-140045},
	doi = {10.1136/postgradmedj-2021-140045},
	abstract = {Reference letters play an important role for both postgraduate residency applications and medical faculty hiring processes. This study seeks to characterise the ways in which gender bias may manifest in the language of reference letters in academic medicine. In particular, we conducted a systematic review in accordance with Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. We searched Embase, MEDLINE and PsycINFO from database inception to July 2020 for original studies that assessed gendered language in medical reference letters for residency applications and medical faculty hiring. A total of 16 studies, involving 12 738 letters of recommendation written for 7074 applicants, were included. A total of 32\% of applicants were women. There were significant differences in how women were described in reference letters. A total of 64\% (7/11) studies found a significant difference in gendered adjectives between men and women. Among the 7 studies, a total of 86\% (6/7) noted that women applicants were more likely to be described using communal adjectives, such as {\textquotedblleft}delightful{\textquotedblright} or {\textquotedblleft}compassionate{\textquotedblright}, while men applicants were more likely to be described using agentic adjectives, such as {\textquotedblleft}leader{\textquotedblright} or {\textquotedblleft}exceptional{\textquotedblright}. Several studies noted that reference letters for women applicants had more frequent use of doubt raisers and mentions of applicant personal life and/or physical appearance. Only one study assessed the outcome of gendered language on application success, noting a higher residency match rate for men applicants. Reference letters within medicine and medical education exhibit language discrepancies between men and women applicants, which may contribute to gender bias against women in medicine.},
	number = {1170},
	urldate = {2025-02-06},
	journal = {Postgraduate Medical Journal},
	author = {Khan, Shawn and Kirubarajan, Abirami and Shamsheri, Tahmina and Clayton, Adam and Mehta, Geeta},
	month = apr,
	year = {2023},
	pages = {272--278},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\ABMNLVG3\\Khan et al. - 2023 - Gender bias in reference letters for residency and.pdf:application/pdf;postgradmedj-2021-140045-dc1-inline-supplementary-material-1.pdf:C\:\\Users\\jana\\Zotero\\storage\\74UITMMV\\postgradmedj-2021-140045-dc1-inline-supplementary-material-1.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\39K5IFMB\\7177392.html:text/html},
}

@mastersthesis{cugno_talk_2020,
	address = {Edwardsville, Illinois, USA},
	title = {Talk {Like} a {Man}: {How} {Resume} {Writing} {Can} {Impact} {Managerial} {Hiring} {Decisions} for {Women}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	shorttitle = {Talk {Like} a {Man}},
	url = {https://www.proquest.com/docview/2410658740/abstract/F57DF69880904C99PQ/1},
	abstract = {The purpose of this study was to examine the impact resume content has on employment decisions. Specifically, this study aims to provide evidence of discrimination against female job applicants applying for leadership roles. Furthermore, this paper discusses the concept of gendered language potentially creating differences between female and male resumes, ultimately impacting women's likelihood of being hired. The current study used the data from 233 Mechanical Turk participants who completed a survey for monetary incentive. It was hypothesized that male job applicants would be preferred over female applicants. It was also hypothesized that agentic language in resumes would be preferred over communal language. Additionally, it was hypothesized that women using agentic language would be rated lower than not only other female applicants using communal language, but also male applicants using agentic language. Agentic language was found to be significantly more hireable than communal language, regardless of gender. Female applicants, as well as those who used communal language in general, were rated higher in warmth than other applicants. Also, male applicants using communal language received the lowest competency ratings out of any other group. Additional findings are discussed.},
	urldate = {2025-02-06},
	school = {Southern Illinois University at Edwardsville},
	author = {Cugno, Melissa},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\PDB7D2PS\\Cugno - 2020 - Talk Like a Man How Resume Writing Can Impact Man.pdf:application/pdf},
}

@article{liu_using_2009,
	title = {Using the {Standardized} {Letters} of {Recommendation} in {Selection}: {Results} {From} a {Multidimensional} {Rasch} {Model}},
	volume = {69},
	issn = {0013-1644},
	shorttitle = {Using the {Standardized} {Letters} of {Recommendation} in {Selection}},
	url = {https://doi.org/10.1177/0013164408322031},
	doi = {10.1177/0013164408322031},
	abstract = {In an effort to standardize academic application procedures, the authors developed the Standardized Letters of Recommendation (SLR) to capture important cognitive and noncognitive qualities of graduate school candidates. The SLR, which consists of seven scales, is applied to an intern-selection scenario. Both professor ratings (n = 414) during the application process and mentor ratings of the selected students (n = 51) are collected using the SLR. A multidimensional Rasch investigation suggests that the SLR displays satisfactory internal consistency, model fit, and item fit. The two cognitive scales, knowledge and analytical skills, are found to be the best predictors for intern selection. The professor ratings are systematically higher than the mentor ratings. Possible reasons for the rating discrepancies are discussed. Also, implications for how the SLR can be used and improved in other selection situations are suggested.},
	number = {3},
	urldate = {2025-02-06},
	journal = {Educational and Psychological Measurement},
	author = {Liu, Ou Lydia and Minsky, Jennifer and Ling, Guangming and Kyllonen, Patrick},
	month = jun,
	year = {2009},
	pages = {475--492},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\AW4PSJRC\\Liu et al. - 2009 - Using the Standardized Letters of Recommendation i.pdf:application/pdf},
}

@article{madera_raising_2019,
	title = {Raising {Doubt} in {Letters} of {Recommendation} for {Academia}: {Gender} {Differences} and {Their} {Impact}},
	volume = {34},
	issn = {1573-353X},
	shorttitle = {Raising {Doubt} in {Letters} of {Recommendation} for {Academia}},
	url = {https://doi.org/10.1007/s10869-018-9541-1},
	doi = {10.1007/s10869-018-9541-1},
	abstract = {The extent of gender bias in academia continues to be an object of inquiry, and recent research has begun to examine the particular gender biases emblematic in letters of recommendations. This current two-part study examines differences in the number of doubt raisers that are written in 624 authentic letters of recommendations for 174 men and women applying for eight assistant professor positions (study 1) and the impact of these doubt raisers on 305 university professors who provided evaluations of recommendation letters (study 2). The results show that both male and female recommenders use more doubt raisers in letters of recommendations for women compared to men and that the presence of certain types of doubt raisers in letters of recommendations results in negative outcomes for both genders. Since doubt raisers are more frequent in letters for women than men, women are at a disadvantage relative to men in their applications for academic positions. We discuss the implications and need for additional future research and practice that (1) raises awareness that letter writers are gatekeepers who can improve or hinder women{\textquoteright}s progress and (2) develops methods to eliminate the skewed use of doubt raisers.},
	number = {3},
	urldate = {2025-02-06},
	journal = {Journal of Business and Psychology},
	author = {Madera, Juan M. and Hebl, Michelle R. and Dial, Heather and Martin, Randi and Valian, Virgina},
	month = jun,
	year = {2019},
	pages = {287--303},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\SWIXJS6X\\Madera et al. - 2019 - Raising Doubt in Letters of Recommendation for Aca.pdf:application/pdf},
}

@article{schmader_linguistic_2007,
	title = {A {Linguistic} {Comparison} of {Letters} of {Recommendation} for {Male} and {Female} {Chemistry} and {Biochemistry} {Job} {Applicants}},
	volume = {57},
	issn = {1573-2762},
	url = {https://doi.org/10.1007/s11199-007-9291-4},
	doi = {10.1007/s11199-007-9291-4},
	abstract = {Letters of recommendation are central to the hiring process. However, gender stereotypes could bias how recommenders describe female compared to male applicants. In the current study, text analysis software was used to examine 886 letters of recommendation written on behalf of 235 male and 42 female applicants for either a chemistry or biochemistry faculty position at a large U.S. research university. Results revealed more similarities than differences in letters written for male and female candidates. However, recommenders used significantly more standout adjectives to describe male as compared to female candidates. Letters containing more standout words also included more ability words and fewer grindstone words. Research is needed to explore how differences in language use affect perceivers{\textquoteright} evaluations of female candidates.},
	number = {7},
	urldate = {2025-02-07},
	journal = {Sex Roles},
	author = {Schmader, Toni and Whitehead, Jessica and Wysocki, Vicki H.},
	month = oct,
	year = {2007},
	pages = {509--514},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\JLM8W5FQ\\Schmader et al. - 2007 - A Linguistic Comparison of Letters of Recommendati.pdf:application/pdf},
}

@article{heilman_why_2007,
	title = {Why are women penalized for success at male tasks?: {The} implied communality deficit.},
	volume = {92},
	issn = {0021-9010},
	shorttitle = {Why are women penalized for success at male tasks?},
	url = {https://research.ebsco.com/linkprocessor/plink?id=8e6ddfa1-9c3c-3469-82d5-f6100d8bb70e},
	doi = {10.1037/0021-9010.92.1.81},
	abstract = {In 3 experimental studies, the authors tested the idea that penalties women incur for success in traditionally male areas arise from a perceived deficit in nurturing and socially sensitive communal attributes that is implied by their success. The authors therefore expected that providing information of communality would prevent these penalties. Results indicated that the negativity directed at successful female managers-in ratings of likability, interpersonal hostility, and boss desirability-was mitigated when there was indication that they were communal. This ameliorative effect occurred only when the information was clearly indicative of communal attributes (Study 1) and when it could be unambiguously attributed to the female manager (Study 2); furthermore, these penalties were averted when communality was conveyed by role information (motherhood status) or by behavior (Study 3). These findings support the idea that penalties for women's success in male domains result from the perceived violation of gender-stereotypic prescriptions. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {1},
	urldate = {2025-02-07},
	journal = {Journal of Applied Psychology},
	author = {Heilman, Madeline E. and Okimoto, Tyler G.},
	month = jan,
	year = {2007},
	pages = {81--92},
	file = {Full text PDF:C\:\\Users\\jana\\Zotero\\storage\\5PDXR349\\Heilman and Okimoto - 2007 - Why are women penalized for success at male tasks.pdf:application/pdf},
}

@article{li_gender_2017,
	title = {Gender {Differences} in {Language} of {Standardized} {Letter} of {Evaluation} {Narratives} for {Emergency} {Medicine} {Residency} {Applicants}},
	volume = {1},
	copyright = {{\textcopyright} 2017 by the Society for Academic Emergency Medicine},
	issn = {2472-5390},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aet2.10057},
	doi = {10.1002/aet2.10057},
	abstract = {Objective While gender differences in language for letters of recommendation have been identified in other fields, no prior studies have evaluated the narrative portion of the emergency medicine (EM) standardized letter of evaluation (SLOE). We aim to examine the differences in language used to describe male and female applicants within the SLOE narrative. Methods Invited applicants to a 4-year academic EM residency program within a single application year with a SLOE were included in the sample. Exclusion criteria were SLOE of applicants from non{\textendash}Liaison Committee on Medical Education (LCME) schools or first rotation SLOE not available for download. Data were collected on applicant gender, age, rotation grade, Alpha Omega Alpha designation, and medical school rank. The previously validated Linguistic Inquiry and Word Count (LIWC) program was used to analyze frequency of words within categories relevant to letters of recommendation. Descriptive statistics, t-tests, and chi-square tests were employed in analysis. Results Of 1,025 applicants within a single application year, 265 were invited to interview; 237 applicants had a first rotation SLOE available for analysis. There were no differences between male and female applicants for baseline characteristics. The median word count per SLOE narrative was 199; within the LIWC dictionary and user-defined categories, words within the categories of affiliation and ability appeared more frequently for female applicants. Conclusions Our results with respect to the SLOE narrative reinforce prior research that letters of recommendation for female applicants highlight communal characteristics of teamwork, helpfulness, and compassion. Contrary to prior research, ability words highlighting intelligence and skill appeared with greater frequency for female applicants. No pervasive differences were found in other word categories. In this sample, the standardized format of the SLOE resulted in letters that were relatively free of gender bias.},
	number = {4},
	urldate = {2025-02-07},
	journal = {AEM Education and Training},
	author = {Li, Simiao and Fant, Abra L. and McCarthy, Danielle M. and Miller, Danielle and Craig, Jill and Kontrick, Amy},
	year = {2017},
	pages = {334--339},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\P3KYXC7K\\Li et al. - 2017 - Gender Differences in Language of Standardized Let.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\JQHSEQRD\\aet2.html:text/html},
}

@article{pietraszkiewicz_big_2019,
	title = {The big two dictionaries: {Capturing} agency and communion in natural language},
	volume = {49},
	copyright = {{\textcopyright} 2018 John Wiley \& Sons, Ltd.},
	issn = {1099-0992},
	shorttitle = {The big two dictionaries},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2561},
	doi = {10.1002/ejsp.2561},
	abstract = {Four studies developed and validated two dictionaries to capture agentic and communal expressions in natural language. Their development followed the Linguistic Inquiry and Word Count (LIWC) approach (Study 1) and we tested their validity with frequency-based analyses and semantic similarity measures. The newly developed Agency and Communion dictionaries were aligned with LIWC categories related to agency and communion (Study 2), and corresponded with subjective ratings (Study 3), confirming their convergent validity. Very low or absent correspondence between proposed dictionaries and unrelated LIWC categories demonstrated their discriminant validity (Study 2). Finally, we applied both dictionaries to language used in advertisements. In correspondence to gender stereotypes, male-dominated jobs were advertised with more agentic than communal words, and female-dominated jobs with more communal than agentic words (Study 4). Both dictionaries represent reliable tools for quantifying agentic and communal content in natural language, and will improve and facilitate future research on agency and communion.},
	number = {5},
	urldate = {2025-02-07},
	journal = {European Journal of Social Psychology},
	author = {Pietraszkiewicz, Agnieszka and Formanowicz, Magdalena and Gustafsson Send{\'e}n, Marie and Boyd, Ryan L. and Sikstr{\"o}m, Sverker and Sczesny, Sabine},
	year = {2019},
	pages = {871--887},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\TTPSYCT5\\ejsp.html:text/html},
}

@article{farlow_gender_2024,
	title = {Gender {Bias} in {Artificial} {Intelligence}-{Written} {Letters} of {Reference}},
	volume = {171},
	copyright = {{\textcopyright} 2024 The Authors. Otolaryngology{\textendash}Head and Neck Surgery published by Wiley Periodicals LLC on behalf of American Academy of Otolaryngology{\textendash}Head and Neck Surgery Foundation.},
	issn = {1097-6817},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ohn.806},
	doi = {10.1002/ohn.806},
	abstract = {Objective Letters of reference (LORs) play an important role in postgraduate residency applications. Human-written LORs have been shown to carry implicit gender bias, such as using more agentic versus communal words for men, and more frequent doubt-raisers and references to appearance and personal life for women. This can result in inequitable access to residency opportunities for women. Given the known gendered language often unconsciously inserted into human-written LORs, we sought to identify whether LORs generated by artificial intelligence exhibit gender bias. Study Design Observational study. Setting Multicenter academic collaboration. Methods Prompts describing identical men and women applying for Otolaryngology residency positions were created and provided to ChatGPT to generate LORs. These letters were analyzed using a gender-bias calculator which assesses the proportion of male- versus female-associated words. Results Regardless of the gender, school, research, or other activities, all LORs generated by ChatGPT showed a bias toward male-associated words. There was no significant difference between the percentage of male-biased words in letters written for women versus men (39.15 vs 37.85, P = .77). There were significant differences in gender bias found by each of the other discrete variables (school, research, and other activities) chosen. Conclusion While ChatGPT-generated LORs all showed a male bias in the language used, there was no gender bias difference in letters produced using traditionally masculine versus feminine names and pronouns. Other variables did induce gendered language, however. ChatGPT is a promising tool for LOR drafting, but users must be aware of potential biases introduced or propagated through these technologies.},
	language = {en},
	number = {4},
	urldate = {2025-02-07},
	journal = {Otolaryngology{\textendash}Head and Neck Surgery},
	author = {Farlow, Janice L. and Abouyared, Marianne and Rettig, Eleni M. and Kejner, Alexandra and Patel, Rusha and Edwards, Heather A.},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/ohn.806},
	pages = {1027--1032},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\Y73AABRW\\Farlow et al. - 2024 - Gender Bias in Artificial Intelligence-Written Let.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\MUQZYG96\\ohn.html:text/html},
}

@article{eagly_leadership_2001,
	title = {The {Leadership} {Styles} of {Women} and {Men}},
	volume = {57},
	copyright = {2001 The Society for the Psychological Study of Social Issues},
	issn = {1540-4560},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/0022-4537.00241},
	doi = {10.1111/0022-4537.00241},
	abstract = {As women increasingly enter leadership roles that traditionally have been occupied mainly by men, the possibility that the leadership styles of women and men differ continues to attract attention. The focus of these debates on sameness versus difference can obscure the array of causal factors that can produce differences or similarities. Adopting the perspective of social role theory, we offer a framework that encompasses many of the complexities of the empirical literature on the leadership styles of women and men. Supplementing Eagly and Johnson's (1990) review of the interpersonally oriented, task-oriented, autocratic, and democratic styles of women and men, we present new data concerning the transformational, transactional, and laissez-faire leadership styles.},
	number = {4},
	urldate = {2025-02-07},
	journal = {Journal of Social Issues},
	author = {Eagly, Alice H. and Johannesen-Schmidt, Mary C.},
	year = {2001},
	pages = {781--797},
	file = {Eagly and Johannesen-Schmidt - 2001 - The Leadership Styles of Women and Men.pdf:C\:\\Users\\jana\\Zotero\\storage\\W8GRE2SI\\Eagly and Johannesen-Schmidt - 2001 - The Leadership Styles of Women and Men.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\YYTBLXIT\\0022-4537.html:text/html},
}

@article{eagly_role_2002,
	title = {Role congruity theory of prejudice toward female leaders.},
	volume = {109},
	issn = {0033-295X},
	url = {https://research.ebsco.com/linkprocessor/plink?id=a0225d92-b825-339e-844a-580f5153bab3},
	doi = {10.1037/0033-295X.109.3.573},
	abstract = {A role congruity theory of prejudice toward female leaders proposes that perceived incongruity between the female gender role and leadership roles leads to 2 forms of prejudice: (a) perceiving women less favorably than men as potential occupants of leadership roles and (b) evaluating behavior that fulfills the prescriptions of a leader role less favorably when it is enacted by a woman. One consequence is that attitudes are less positive toward female than male leaders and potential leaders. Other consequences are that it is more difficult for women to become leaders and to achieve success in leadership roles. Evidence from varied research paradigms substantiates that these consequences occur, especially in situations that heighten perceptions of incongruity between the female gender role and leadership roles. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {3},
	urldate = {2025-02-07},
	journal = {Psychological Review},
	author = {Eagly, Alice H. and Karau, Steven J.},
	month = jul,
	year = {2002},
	pages = {573--598},
	file = {Full text PDF:C\:\\Users\\jana\\Zotero\\storage\\ZYA3855N\\Eagly and Karau - 2002 - Role congruity theory of prejudice toward female l.pdf:application/pdf},
}

@article{wood_cross-cultural_2002,
	title = {A cross-cultural analysis of the behavior of women and men: {Implications} for the origins of sex differences.},
	volume = {128},
	issn = {0033-2909},
	shorttitle = {A cross-cultural analysis of the behavior of women and men},
	url = {https://research.ebsco.com/linkprocessor/plink?id=7f9fafbc-0c9e-38dd-9d22-87d4e4e3ab6e},
	doi = {10.1037/0033-2909.128.5.699},
	abstract = {This article evaluates theories of the origins of sex differences in human behavior. It reviews the cross-cultural evidence on the behavior of women and men in nonindustrial societies, especially the activities that contribute to the sex-typed division of labor and patriarchy. To explain the cross-cultural findings, the authors consider social constructionism, evolutionary psychology, and their own biosocial theory. Supporting the biosocial analysis, sex differences derive from the interaction between the physical specialization of the sexes, especially female reproductive capacity, and the economic and social structural aspects of societies. This biosocial approach treats the psychological attributes of women and men as emergent given the evolved characteristics of the sexes, their developmental experiences, and their situated activity in society. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
	number = {5},
	urldate = {2025-02-07},
	journal = {Psychological Bulletin},
	author = {Wood, Wendy and Eagly, Alice H.},
	month = sep,
	year = {2002},
	pages = {699--727},
	file = {Full text PDF:C\:\\Users\\jana\\Zotero\\storage\\ZEQZ9JT9\\Wood and Eagly - 2002 - A cross-cultural analysis of the behavior of women.pdf:application/pdf},
}

@article{trix_exploring_2003,
	title = {Exploring the {Color} of {Glass}: {Letters} of {Recommendation} for {Female} and {Male} {Medical} {Faculty}},
	volume = {14},
	issn = {0957-9265},
	shorttitle = {Exploring the {Color} of {Glass}},
	url = {https://doi.org/10.1177/0957926503014002277},
	doi = {10.1177/0957926503014002277},
	abstract = {This study examines over 300 letters of recommendation for medical faculty at a large American medical school in the mid-1990s, using methods from corpus and discourse analysis, with the theoretical perspective of gender schema from cognitive psychology. Letters written for female applicants were found to differ systematically from those written for male applicants in the extremes of length, in the percentages lacking in basic features, in the percentages with doubt raisers (an extended category of negative language, often associated with apparent commendation), and in frequency of mention of status terms. Further, the most common semantically grouped possessive phrases referring to female and male applicants (`her teaching,' `his research') reinforce gender schema that tend to portray women as teachers and students, and men as researchers and professionals.},
	language = {en},
	number = {2},
	urldate = {2025-02-10},
	journal = {Discourse \& Society},
	author = {Trix, Frances and Psenka, Carolyn},
	month = mar,
	year = {2003},
	note = {Publisher: SAGE Publications Ltd},
	pages = {191--220},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\3APRGSYT\\TRIX and PSENKA - 2003 - Exploring the Color of Glass Letters of Recommend.pdf:application/pdf},
}

@article{nemani_gender_2024,
	title = {Gender bias in transformers: {A} comprehensive review of detection and mitigation strategies},
	volume = {6},
	issn = {2949-7191},
	shorttitle = {Gender bias in transformers},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719123000444},
	doi = {10.1016/j.nlp.2023.100047},
	abstract = {Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals{\textquoteright} lives. This paper presents a comprehensive survey that explores gender bias in Transformer models from a linguistic perspective. While the existence of gender bias in language models has been acknowledged in previous studies, there remains a lack of consensus on how to measure and evaluate this bias effectively. Our survey critically examines the existing literature on gender bias in Transformers, shedding light on the diverse methodologies and metrics employed to assess bias. Several limitations in current approaches to measuring gender bias in Transformers are identified, encompassing the utilization of incomplete or flawed metrics, inadequate dataset sizes, and a dearth of standardization in evaluation methods. Furthermore, our survey delves into the potential ramifications of gender bias in Transformers for downstream applications, including dialogue systems and machine translation. We underscore the importance of fostering equity and fairness in these systems by emphasizing the need for heightened awareness and accountability in developing and deploying language technologies. This paper serves as a comprehensive overview of gender bias in Transformer models, providing novel insights and offering valuable directions for future research in this critical domain.},
	urldate = {2025-02-12},
	journal = {Natural Language Processing Journal},
	author = {Nemani, Praneeth and Joel, Yericherla Deepak and Vijay, Palla and Liza, Farhana Ferdouzi},
	month = mar,
	year = {2024},
	pages = {100047},
	file = {Accepted Version:C\:\\Users\\jana\\Zotero\\storage\\QULFUWZK\\Nemani et al. - 2024 - Gender bias in transformers A comprehensive revie.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\LDSHWJDU\\S2949719123000444.html:text/html},
}

@inproceedings{jiang_personallm_2024,
	address = {Mexico City, Mexico},
	title = {{PersonaLLM}: {Investigating} the {Ability} of {Large} {Language} {Models} to {Express} {Personality} {Traits}},
	shorttitle = {{PersonaLLM}},
	url = {https://aclanthology.org/2024.findings-naacl.229/},
	doi = {10.18653/v1/2024.findings-naacl.229},
	abstract = {Despite the many use cases for large language models (LLMs) in creating personalized chatbots, there has been limited research on evaluating the extent to which the behaviors of personalized LLMs accurately and consistently reflect specific personality traits. We consider studying the behavior of LLM-based agents which we refer to as LLM personas and present a case study with GPT-3.5 and GPT-4 to investigate whether LLMs can generate content that aligns with their assigned personality profiles. To this end, we simulate distinct LLM personas based on the Big Five personality model, have them complete the 44-item Big Five Inventory (BFI) personality test and a story writing task, and then assess their essays with automatic and human evaluations. Results show that LLM personas' self-reported BFI scores are consistent with their designated personality types, with large effect sizes observed across five traits. Additionally, LLM personas' writings have emerging representative linguistic patterns for personality traits when compared with a human writing corpus. Furthermore, human evaluation shows that humans can perceive some personality traits with an accuracy of up to 80\%. Interestingly, the accuracy drops significantly when the annotators were informed of AI authorship.},
	urldate = {2025-02-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Jiang, Hang and Zhang, Xiajie and Cao, Xubo and Breazeal, Cynthia and Roy, Deb and Kabbara, Jad},
	editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
	month = jun,
	year = {2024},
	keywords = {silicon sampling},
	pages = {3605--3627},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\IBZ99C5P\\Jiang et al. - 2024 - PersonaLLM Investigating the Ability of Large Lan.pdf:application/pdf},
}

@inproceedings{wang_my_2024,
	address = {Bangkok, Thailand},
	title = {{\textquotedblleft}{My} {Answer} is {C}{\textquotedblright}: {First}-{Token} {Probabilities} {Do} {Not} {Match} {Text} {Answers} in {Instruction}-{Tuned} {Language} {Models}},
	shorttitle = {{\textquotedblleft}{My} {Answer} is {C}{\textquotedblright}},
	url = {https://aclanthology.org/2024.findings-acl.441/},
	doi = {10.18653/v1/2024.findings-acl.441},
	abstract = {The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model`s diverse response styles such as starting with {\textquotedblleft}Sure{\textquotedblright} or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned on all dimensions, reaching mismatch rates over 60\%. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation.},
	urldate = {2025-02-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xinpeng and Ma, Bolei and Hu, Chengzhi and Weber-Genzel, Leon and R{\"o}ttger, Paul and Kreuter, Frauke and Hovy, Dirk and Plank, Barbara},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {7407--7416},
	file = {Wang et al. - 2024 - {\textquotedblleft}My Answer is C{\textquotedblright} First-Token Probabilities Do Not.pdf:C\:\\Users\\jana\\Zotero\\storage\\U4ZKVTIN\\Wang et al. - 2024 - {\textquotedblleft}My Answer is C{\textquotedblright} First-Token Probabilities Do Not.pdf:application/pdf},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-02-19},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\NAPIMRLQ\\Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot A.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\G6LJ2Q9H\\2306.html:text/html},
}

@book{cohen_statistical_1988,
	address = {New York, NY, USA},
	edition = {2nd},
	title = {Statistical {Power} {Analysis} for the {Behavioral} {Sciences}},
	isbn = {978-0-203-77158-7},
	abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
	publisher = {Routledge},
	author = {Cohen, Jacob},
	year = {1988},
	doi = {10.4324/9780203771587},
}

@misc{huang_who_2024,
	title = {Who is {ChatGPT}? {Benchmarking} {LLMs}' {Psychological} {Portrayal} {Using} {PsychoBench}},
	shorttitle = {Who is {ChatGPT}?},
	url = {http://arxiv.org/abs/2310.01386},
	doi = {10.48550/arXiv.2310.01386},
	abstract = {Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, gpt-3.5-turbo, gpt-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Huang, Jen-tse and Wang, Wenxuan and Li, Eric John and Lam, Man Ho and Ren, Shujie and Yuan, Youliang and Jiao, Wenxiang and Tu, Zhaopeng and Lyu, Michael R.},
	month = jan,
	year = {2024},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\CSERQDZ4\\Huang et al. - 2024 - Who is ChatGPT Benchmarking LLMs' Psychological P.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\ZILB57BG\\2310.html:text/html},
}

@inproceedings{dorner_personality_2023,
	title = {Do {Personality} {Tests} {Generalize} to {Large} {Language} {Models}?},
	url = {https://openreview.net/forum?id=zKDSfGhCoK},
	abstract = {With large language models (LLMs) appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate various properties of these models using tests originally designed for humans. While re-using existing tests is a resource-efficient way to evaluate LLMs, careful adjustments are usually required to ensure that test results are even valid across human sub-populations. Thus, it is not clear to what extent different tests{\textquoteright} validity generalizes to LLMs. In this work, we provide evidence that LLMs{\textquoteright} responses to personality tests systematically deviate from typical human responses, implying that these results cannot be interpreted in the same way as human test results. Concretely, reverse-coded items (e.g. {\textquotedblleft}I am introverted{\textquotedblright} vs {\textquotedblleft}I am extraverted{\textquotedblright}) are often both answered affirmatively by LLMs. In addition, variation across different prompts designed to {\textquotedblleft}steer{\textquotedblright} LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe it is important to pay more attention to tests{\textquoteright} validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs{\textquoteright} {\textquotedblleft}personality{\textquotedblright}.},
	urldate = {2025-03-03},
	booktitle = {Socially {Responsible} {Language} {Modelling} {Research}},
	author = {Dorner, Florian and S{\"u}hr, Tom and Samadi, Samira and Kelava, Augustin},
	month = nov,
	year = {2023},
	keywords = {criticism, population},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\THYEUTZ4\\Dorner et al. - 2023 - Do Personality Tests Generalize to Large Language .pdf:application/pdf},
}

@misc{suhr_challenging_2024,
	title = {Challenging the {Validity} of {Personality} {Tests} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2311.05297},
	doi = {10.48550/arXiv.2311.05297},
	abstract = {With large language models (LLMs) like GPT-4 appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate personality traits of LLMs using questionnaires originally developed for humans. While reusing measures is a resource-efficient way to evaluate LLMs, careful adaptations are usually required to ensure that assessment results are valid even across human subpopulations. In this work, we provide evidence that LLMs' responses to personality tests systematically deviate from human responses, implying that the results of these tests cannot be interpreted in the same way. Concretely, reverse-coded items ("I am introverted" vs. "I am extraverted") are often both answered affirmatively. Furthermore, variation across prompts designed to "steer" LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe that it is important to investigate tests' validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs' "personality".},
	urldate = {2025-03-03},
	publisher = {arXiv},
	author = {S{\"u}hr, Tom and Dorner, Florian E. and Samadi, Samira and Kelava, Augustin},
	month = jun,
	year = {2024},
	keywords = {criticism},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\TR5HD6ZL\\S{\"u}hr et al. - 2024 - Challenging the Validity of Personality Tests for .pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\SA4BQH7J\\2311.html:text/html},
}

@article{nadeem_gender_2020,
	title = {Gender {Bias} in {AI}: {A} {Review} of {Contributing} {Factors} and {Mitigating} {Strategies}},
	shorttitle = {Gender {Bias} in {AI}},
	url = {https://aisel.aisnet.org/acis2020/27},
	journal = {ACIS 2020 Proceedings},
	author = {Nadeem, Ayesha and Abedin, Babak and Marjanovic, Olivera},
	month = jan,
	year = {2020},
	file = {"Gender Bias in AI\: A Review of Contributing Factors and Mitigating Str" by Ayesha Nadeem, Babak Abedin et al.:C\:\\Users\\jana\\Zotero\\storage\\GSMGMRTL\\27.html:text/html;Nadeem et al. - 2020 - Gender Bias in AI A Review of Contributing Factor.pdf:C\:\\Users\\jana\\Zotero\\storage\\9E3ATEK2\\Nadeem et al. - 2020 - Gender Bias in AI A Review of Contributing Factor.pdf:application/pdf},
}

@article{kim_mdagents_2024,
	title = {{MDAgents}: {An} {Adaptive} {Collaboration} of {LLMs} for {Medical} {Decision}-{Making}},
	volume = {37},
	shorttitle = {{MDAgents}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/90d1fc07f46e31387978b88e7e057a31-Abstract-Conference.html},
	urldate = {2025-03-14},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik S. and Xu, Xuhai and McDuff, Daniel and Lee, Hyeonhoon and Ghassemi, Marzyeh and Breazeal, Cynthia and Park, Hae W.},
	month = dec,
	year = {2024},
	pages = {79410--79452},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\H4XKP8ZT\\Kim et al. - 2024 - MDAgents An Adaptive Collaboration of LLMs for Me.pdf:application/pdf},
}

@article{gumilar_assessment_2024,
	title = {Assessment of {Large} {Language} {Models} ({LLMs}) in decision-making support for gynecologic oncology},
	volume = {23},
	issn = {2001-0370},
	url = {https://www.sciencedirect.com/science/article/pii/S2001037024003702},
	doi = {10.1016/j.csbj.2024.10.050},
	abstract = {Objective
This study investigated the ability of Large Language Models (LLMs) to provide accurate and consistent answers by focusing on their performance in complex gynecologic cancer cases.
Background
LLMs are advancing rapidly and require a thorough evaluation to ensure that they can be safely and effectively used in clinical decision-making. Such evaluations are essential for confirming LLM reliability and accuracy in supporting medical professionals in casework.
Study design
We assessed three prominent LLMs{\textemdash}ChatGPT-4 (CG-4), Gemini Advanced (GemAdv), and Copilot{\textemdash}evaluating their accuracy, consistency, and overall performance. Fifteen clinical vignettes of varying difficulty and five open-ended questions based on real patient cases were used. The responses were coded, randomized, and evaluated blindly by six expert gynecologic oncologists using a 5-point Likert scale for relevance, clarity, depth, focus, and coherence.
Results
GemAdv demonstrated superior accuracy (81.87~\%) compared to both CG-4 (61.60~\%) and Copilot (70.67~\%) across all difficulty levels. GemAdv consistently provided correct answers more frequently ({\textgreater}60~\% every day during the testing period). Although CG-4 showed a slight advantage in adhering to the National Comprehensive Cancer Network (NCCN) treatment guidelines, GemAdv excelled in the depth and focus of the answers provided, which are crucial aspects of clinical decision-making.
Conclusion
LLMs, especially GemAdv, show potential in supporting clinical practice by providing accurate, consistent, and relevant information for gynecologic cancer. However, further refinement is needed for more complex scenarios. This study highlights the promise of LLMs in gynecologic oncology, emphasizing the need for ongoing development and rigorous evaluation to maximize their clinical utility and reliability.},
	urldate = {2025-03-14},
	journal = {Computational and Structural Biotechnology Journal},
	author = {Gumilar, Khanisyah Erza and Indraprasta, Birama R. and Faridzi, Ach Salman and Wibowo, Bagus M. and Herlambang, Aditya and Rahestyningtyas, Eccita and Irawan, Budi and Tambunan, Zulkarnain and Bustomi, Ahmad Fadhli and Brahmantara, Bagus Ngurah and Yu, Zih-Ying and Hsu, Yu-Cheng and Pramuditya, Herlangga and Putra, Very Great E. and Nugroho, Hari and Mulawardhana, Pungky and Tjokroprawiro, Brahmana A. and Hedianto, Tri and Ibrahim, Ibrahim H. and Huang, Jingshan and Li, Dongqi and Lu, Chien-Hsing and Yang, Jer-Yen and Liao, Li-Na and Tan, Ming},
	month = dec,
	year = {2024},
	pages = {4019--4026},
	file = {ScienceDirect Snapshot:C\:\\Users\\jana\\Zotero\\storage\\AW6QDQJZ\\S2001037024003702.html:text/html},
}

@article{greenwald_measuring_1998,
	title = {Measuring individual differences in implicit cognition: the implicit association test},
	volume = {74},
	issn = {0022-3514},
	shorttitle = {Measuring individual differences in implicit cognition},
	doi = {10.1037//0022-3514.74.6.1464},
	abstract = {An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect \& pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects).},
	number = {6},
	journal = {Journal of Personality and Social Psychology},
	author = {Greenwald, A. G. and McGhee, D. E. and Schwartz, J. L.},
	month = jun,
	year = {1998},
	pages = {1464--1480},
}

@inproceedings{may_measuring_2019,
	address = {Minneapolis, Minnesota, USA},
	title = {On {Measuring} {Social} {Biases} in {Sentence} {Encoders}},
	volume = {1},
	url = {http://arxiv.org/abs/1903.10561},
	doi = {10.48550/arXiv.1903.10561},
	abstract = {The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test's assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.},
	urldate = {2023-10-11},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {May, Chandler and Wang, Alex and Bordia, Shikha and Bowman, Samuel R. and Rudinger, Rachel},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = mar,
	year = {2019},
	pages = {622--628},
	file = {arXiv Fulltext PDF:C\:\\Users\\jana\\Zotero\\storage\\69K553QU\\May et al. - 2019 - On Measuring Social Biases in Sentence Encoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jana\\Zotero\\storage\\68ZWUE2B\\1903.html:text/html},
}

@inproceedings{nangia_crows-pairs_2020,
	address = {Online},
	title = {{CrowS}-{Pairs}: {A} {Challenge} {Dataset} for {Measuring} {Social} {Biases} in {Masked} {Language} {Models}},
	shorttitle = {{CrowS}-{Pairs}},
	url = {https://aclanthology.org/2020.emnlp-main.154},
	doi = {10.18653/v1/2020.emnlp-main.154},
	abstract = {Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.},
	urldate = {2024-01-09},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel R.},
	editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
	month = nov,
	year = {2020},
	pages = {1953--1967},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\6Q98N73W\\Nangia et al. - 2020 - CrowS-Pairs A Challenge Dataset for Measuring Soc.pdf:application/pdf},
}

@article{pistella_sexism_2018,
	title = {Sexism and {Attitudes} {Toward} {Same}-{Sex} {Parenting} in a {Sample} of {Heterosexuals} and {Sexual} {Minorities}: the {Mediation} {Effect} of {Sexual} {Stigma}},
	volume = {15},
	issn = {1553-6610},
	shorttitle = {Sexism and {Attitudes} {Toward} {Same}-{Sex} {Parenting} in a {Sample} of {Heterosexuals} and {Sexual} {Minorities}},
	url = {https://doi.org/10.1007/s13178-017-0284-y},
	doi = {10.1007/s13178-017-0284-y},
	abstract = {The present study aimed to: (a) investigate the relationship between attitudes toward same-sex parenting and sexism both in heterosexuals and sexual minorities; (b) verify whether sexism predicted negative attitudes toward same-sex parenting via the mediating role of sexual stigma (sexual prejudice in heterosexual people and internalized sexual stigma [ISS] in lesbians and gay men [LG]). An Italian sample of 477 participants (65.6\% heterosexual people and 34.4\% LG people) was used to verify three hypotheses: (a) heterosexual men showed higher levels of sexism than heterosexual women and LG people; (b) heterosexual men reported more negative attitudes toward same-sex parenting than those of heterosexual women and LG people; and (c) sexual prejudice in heterosexual people and ISS in LG people mediated the relationship between sexism and attitudes toward same-sex parenting. Overall, men and heterosexual people showed stronger sexist tendencies and more negative attitudes toward same-sex parenting. Moreover, sexism affected attitudes toward same-sex parenting via sexual prejudice in heterosexual people and ISS in LG people. These results suggest that negative attitudes toward same-sex parenting reflect sociocultural inequalities based on the traditional gender belief system and points to the necessity of social policies to reduce prejudice toward sexual minority groups.},
	number = {2},
	urldate = {2025-03-15},
	journal = {Sexuality Research and Social Policy},
	author = {Pistella, Jessica and Tanzilli, Annalisa and Ioverno, Salvatore and Lingiardi, Vittorio and Baiocco, Roberto},
	month = jun,
	year = {2018},
	pages = {139--150},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\9D3WAQZU\\Pistella et al. - 2018 - Sexism and Attitudes Toward Same-Sex Parenting in .pdf:application/pdf},
}

@article{morrison_construction_2007,
	title = {The {Construction} and {Validation} of the {Homopositivity} {Scale}},
	volume = {52},
	issn = {0091-8369},
	url = {https://www.tandfonline.com/doi/abs/10.1300/J082v52n03_04},
	doi = {10.1300/J082v52n03_04},
	number = {3-4},
	urldate = {2025-03-15},
	journal = {Journal of Homosexuality},
	author = {Morrison, Todd G. and Bearden, Anomi G.},
	month = may,
	year = {2007},
	pages = {63--89},
}

@article{masser_contemporary_1999,
	title = {Contemporary {Sexism}: {The} {Relationships} {Among} {Hostility}, {Benevolence}, and {Neosexism}},
	volume = {23},
	issn = {0361-6843},
	shorttitle = {Contemporary {Sexism}},
	url = {https://doi.org/10.1111/j.1471-6402.1999.tb00378.x},
	doi = {10.1111/j.1471-6402.1999.tb00378.x},
	abstract = {Previous research has examined the relationship between the Modern Sexism Scale (Swim, Aikin, Hall \& Hunter, 1995) and the Ambivalent Sexism Inventory (ASI; Glick \& Fiske, 1996) or the relationship between the Modern Sexism Scale and the Neosexism Scale (Campbell, Schellenberg, \& Senn, 1997; Tougas, Brown, Beaton, \& Joly, 1995). The present study examined the relationship between the ASI and the Neosexism Scale. Across three samples (N = 907), neosexism was found to be associated primarily with the hostile rather than the benevolent component of ambivalent sexism. Hostile sexism, benevolent sexism, and neosexism were all significantly associated with attitudes toward lesbians' and gay men's rights, and both hostile sexism and neosexism were significantly associated with attitudes toward women's rights. Neosexism and hostile sexism were negatively associated with a humanitarian-egalitarian orientation, whereas benevolent sexism was positively associated with a Protestant-ethic orientation. It is concluded that, although both the Neosexism Scale and ASI measure contemporary sexism, only the Benevolent Sexism subscale of the ASI taps the subjectively positive side of contemporary sexism.},
	number = {3},
	urldate = {2025-03-15},
	journal = {Psychology of Women Quarterly},
	author = {Masser, Barbara and Abrams, Dominic},
	month = sep,
	year = {1999},
	pages = {503--517},
}

@article{angelone_mens_2015,
	title = {Men{\textquoteright}s {Perceptions} of an {Acquaintance} {Rape}: {The} {Role} of {Relationship} {Length}, {Victim} {Resistance}, and {Gender} {Role} {Attitudes}},
	volume = {30},
	issn = {0886-2605},
	shorttitle = {Men{\textquoteright}s {Perceptions} of an {Acquaintance} {Rape}},
	url = {https://doi.org/10.1177/0886260514552448},
	doi = {10.1177/0886260514552448},
	abstract = {Sexual aggression is a persistent and prevalent issue in the United States, which often results in a number of psychological, emotional, and physical consequences for victims. The current study examined whether the length of relationship between the victim and perpetrator, level of victim resistance, and observers{\textquoteright} gender role attitudes play a role in observers{\textquoteright} perceptions of an alleged sexual assault. Participants included 297 male college students from a public university in the Northeastern United States. Contrary to hypotheses, there were no significant effects for length of relationship on participants{\textquoteright} attributions. Relative to no resistance, verbal and physical strategies by the victim predicted higher levels of victim credibility, perpetrator culpability, and perpetrator guilt, as well as lower levels of victim culpability and perceived victim pleasure. Endorsement of traditional adversarial sex role beliefs and hostile sexist attitudes, as opposed to egalitarian attitudes, were associated with the attribution of less credibility to the victim, perceived victim trauma, perpetrator culpability, perpetrator guilt, and shorter recommended prison sentences, as well as greater victim culpability and perceived victim pleasure. Laypersons{\textquoteright} perceptions of sexual assault merit further study, as they are relevant to juror decision making and third party responses to sexual victimization (e.g., peer support for victim) and can contribute to the secondary victimization and recovery of survivors of sexual assault.},
	number = {13},
	urldate = {2025-03-15},
	journal = {Journal of Interpersonal Violence},
	author = {Angelone, D. J. and Mitchell, Damon and Grossi, Laura},
	month = aug,
	year = {2015},
	note = {Publisher: SAGE Publications Inc},
	pages = {2278--2303},
}

@article{persson_moderating_2022,
	title = {Moderating {Factors} in {Culpability} {Ratings} and {Rape} {Proclivity} in {Stranger} and {Acquaintance} {Rape}: {Validation} of {Rape} {Vignettes} in a {Community} {Sample}},
	volume = {37},
	issn = {0886-2605},
	shorttitle = {Moderating {Factors} in {Culpability} {Ratings} and {Rape} {Proclivity} in {Stranger} and {Acquaintance} {Rape}},
	url = {https://doi.org/10.1177/0886260521991294},
	doi = {10.1177/0886260521991294},
	abstract = {Rape is a serious concern globally. Past research has identified Ambivalent Sexism (AS), Rape Myth Acceptance (RMA), and the victim{\textendash}perpetrator relationship as key constructs influencing rape blame attributions and rape proclivity. Limitations with methodologies have, however, limited the practical implications of past research, particularly in the context of underpowered samples and a lack of transparency in vignette development and implementation. In the current research, three studies aimed to validate material to be used in research into rape perceptions and to examine the impact of victim{\textendash}perpetrator relationship, AS, and RMA on victim and perpetrator culpability, and rape proclivity, using an experimental design. On 563 participants, this research developed and validated six rape vignettes which accounted for methodological limitations of past research (Study One) and were found to be believable and realistic by participants; it further found that aggressively sexist attitudes were associated with increased victim culpability and decreased perpetrator culpability (Study Two), and increased rape proclivity (Study Three). Scenarios of a casual acquaintance produced the highest levels of victim culpability and the lowest levels of perpetrator culpability. Victims were ascribed more control than blame, or responsibility. Men reported the highest levels of rape proclivity in scenarios of casual acquaintance, and intimate partner relationships. Contrary to past research, Benevolent Sexism (BS) did not directly impact attributions in rape cases but may maintain and legitimize the attitudes, which do. As some of our findings contradict past research, we suggest that the need for standardized rape vignettes is evident, along with greater transparency and methodological rigor in sexual assault research, as this will improve the practical implications of findings. Reproducible research practices may be useful for this. While limited in diversity, this research has important implications for policy and research practice, particularly in producing validated material that can be reused by future researchers.},
	number = {13-14},
	urldate = {2025-03-15},
	journal = {Journal of Interpersonal Violence},
	author = {Persson, Sofia and Dhingra, Katie},
	month = jul,
	year = {2022},
	note = {Publisher: SAGE Publications Inc},
	pages = {NP11358--NP11385},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\BNZJCHZU\\Persson and Dhingra - 2022 - Moderating Factors in Culpability Ratings and Rape.pdf:application/pdf},
}

@article{lemaire_labeling_2016,
	title = {Labeling {Sexual} {Victimization} {Experiences}: {The} {Role} of {Sexism}, {Rape} {Myth} {Acceptance}, and {Tolerance} for {Sexual} {Harassment}},
	volume = {31},
	issn = {0886-6708, 1945-7073},
	shorttitle = {Labeling {Sexual} {Victimization} {Experiences}},
	url = {https://connect.springerpub.com/content/sgrvv/31/2/332},
	doi = {10.1891/0886-6708.VV-D-13-00148},
	abstract = {{\textless}p{\textgreater}This study investigated whether attitudinal variables, such as benevolent and hostile sexism toward men and women, female rape myth acceptance, and tolerance of sexual harassment are related to women labeling their sexual assault experiences as rape. In a sample of 276 female college students, 71 (25.7\%) reported at least one experience that met the operational definition of rape, although only 46.5\% of those women labeled the experience {\textquotedblleft}rape.{\textquotedblright} Benevolent sexism, tolerance of sexual harassment, and rape myth acceptance, but not hostile sexism, significantly predicted labeling of previous sexual assault experiences by the victims. Specifically, those with more benevolent sexist attitudes toward both men and women, greater rape myth acceptance, and more tolerant attitudes of sexual harassment were less likely to label their past sexual assault experience as rape. The results are discussed for their clinical and theoretical implications.{\textless}/p{\textgreater}},
	number = {2},
	urldate = {2025-03-15},
	journal = {Violence and Victims},
	author = {LeMaire, Kelly L. and Oswald, Debra L. and Russell, Brenda L.},
	month = jan,
	year = {2016},
	pages = {332--346},
}

@article{juarros-basterretxea_considering_2019,
	title = {Considering the {Effect} of {Sexism} on {Psychological} {Intimate} {Partner} {Violence}: {A} {Study} with {Imprisoned} {Men}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/deed.es\_ES},
	issn = {1889-1861},
	shorttitle = {Considering the {Effect} of {Sexism} on {Psychological} {Intimate} {Partner} {Violence}},
	url = {https://journals.copmadrid.org/ejpalc/art/ejpalc2019a1},
	doi = {10.5093/ejpalc2019a1},
	abstract = {Psychological intimate partner violence (IPV) is the most prevalent form of IPV and is often thought to precede physical IPV. However, psychological IPV often occurs independently of other forms of IPV, and it can often emerge during routine relationship interactions. Using data from imprisoned male offenders we investigate the effect of hostile and benevolent sexist attitudes on psychological IPV and the hypothesized mediating role of positive attitudes toward IPV and this effect when accounting for broader risk factors at the levels of community (social disorder), family-of-origin (conflictive climate in family of origin), and personality (antisocial personality traits) variables. The sample involved 196 male inmates of the Penitentiary Center of Villabona (Asturias, Spain). Structural equation models result showed significant total, direct and indirect effect of hostile sexism on psychological IPV, but not of benevolent sexism. When individual, family-of-origin, and community variables were considered, however, hostile sexism showed only an indirect effect on psychological IPV via positive attitudes toward abuse. These results are discussed in light of the debate of the role of sexist attitudes in the psychological IPV explanation when broader models are considered.},
	number = {2},
	urldate = {2025-03-15},
	journal = {European Journal of Psychology Applied to Legal Context},
	author = {Juarros-Basterretxea, Joel and Overall, Nickola and Herrero, Juan and Rodr{\'i}guez-D{\'i}az, Francisco J.},
	month = jun,
	year = {2019},
	pages = {61--69},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\CLTT2TRR\\Juarros-Basterretxea et al. - 2019 - Considering the Effect of Sexism on Psychological .pdf:application/pdf},
}

@article{zapata-calvente_automatic_2019,
	title = {Automatic {Associations} and {Conscious} {Attitudes} {Predict} {Different} {Aspects} of {Men}{\textquoteright}s {Intimate} {Partner} {Violence} and {Sexual} {Harassment} {Proclivities}},
	volume = {81},
	issn = {1573-2762},
	url = {https://doi.org/10.1007/s11199-019-1006-0},
	doi = {10.1007/s11199-019-1006-0},
	abstract = {Intimate partner violence against women (IPV) and sexual harassment are both widespread. Research on their causes and attitudinal correlates has rarely examined implicit, automatic cognitive associations related to the partner (in IPV aggressors) or to women (in sexual harassment offenders). The aim of the present research was to study these implicit associations in 129 male German students. Participants completed scales of hostile sexism (HS), masculine gender role stress (MGRS), short-term (STMO) and long-term mating orientation (LTMO), and proclivity to both IPV and sexual harassment. Next they performed a primed lexical decision task that measured whether concepts of violence, power, hostility, and sexuality were differentially associated with representations of women, men, and the participant{\textquoteright}s own intimate partner. Results showed that implicit associations of own partner with violence as well as hostility were generally high but did not correlate strongly with the proclivity measures. Furthermore, the proclivity measures were positively predicted by HS, MGRS, and STMO, whereas LTMO negatively predicted IPV proclivity. Practice implications point to the need to address early socialization processes that may shape men{\textquoteright}s negative associations with female partners. Some strategies to prevent and reduce these types of implicit associations are discussed.},
	number = {7},
	urldate = {2025-03-15},
	journal = {Sex Roles},
	author = {Zapata-Calvente, Antonella L. and Moya, Miguel and Bohner, Gerd and Meg{\'i}as, Jes{\'u}s L.},
	month = oct,
	year = {2019},
	pages = {439--455},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\CSP8ZXML\\Zapata-Calvente et al. - 2019 - Automatic Associations and Conscious Attitudes Pre.pdf:application/pdf},
}

@article{duran_its_2011,
	title = {It's {His} {Right}, {It}'s {Her} {Duty}: {Benevolent} {Sexism} and the {Justification} of {Traditional} {Sexual} {Roles}},
	volume = {48},
	issn = {0022-4499},
	shorttitle = {It's {His} {Right}, {It}'s {Her} {Duty}},
	url = {https://doi.org/10.1080/00224499.2010.513088},
	doi = {10.1080/00224499.2010.513088},
	abstract = {This study tested the effects on social perceptions of sexual marital rights and duties of ambivalent sexist ideology and information about the benevolent sexist ideology of a husband, portrayed in a hypothetical marital vignette. In addition, the perception of whether hypothetical forced sex between husband and wife is considered rape was explored. For one half of the participants (college students), the husband was presented as high in benevolent sexism (BS); and for the other half, no information about his ideology was given. Results showed that participants in the first group ranked sexual marital rights (for him) and duties (for her) more highly, and regarded forced sex as rape to a lesser extent. Positive relationships were also found between participants{\textquoteright} BS and these ratings. Moreover, participants{\textquoteright} perceptions of marital rights and duties played a mediating role in the relationship between their BS and their perception of forced penetration as rape. Finally, an interaction was found between participants{\textquoteright} and husbands{\textquoteright} BS in the perception of marital rights and duties: The influence of participants{\textquoteright} BS was higher when the husband was presented as a benevolent sexist man. Results highlight the role of sexist attitudes in the interpretation of marital rape.},
	number = {5},
	urldate = {2025-03-15},
	journal = {The Journal of Sex Research},
	author = {Dur{\'a}n, Mercedes and Moya, Miguel and Meg{\'i}as, Jes{\'u}s L.},
	month = sep,
	year = {2011},
	pages = {470--478},
}

@article{masser_reinforcing_2004,
	title = {Reinforcing the {Glass} {Ceiling}: {The} {Consequences} of {Hostile} {Sexism} for {Female} {Managerial} {Candidates}},
	volume = {51},
	issn = {1573-2762},
	shorttitle = {Reinforcing the {Glass} {Ceiling}},
	url = {https://doi.org/10.1007/s11199-004-5470-8},
	doi = {10.1007/s11199-004-5470-8},
	abstract = {Previous research has established that benevolent sexism is related to the negative evaluation of women who violate specific norms for behavior. Research has yet to document the causal impact of hostile sexism on evaluations of individual targets. Correlational evidence and ambivalent sexism theory led us to predict that hostile sexism would be associated with negative evaluations of a female candidate for a masculine-typed occupational role. Participants completed the ASI (P. Glick \& S. T. Fiske, 1996) and evaluated a curriculum vitae from either a male or female candidate. Higher hostile sexism was significantly associated with more negative evaluations of the female candidate and with lower recommendations that she be employed as a manager. Conversely, higher hostile sexism was significantly associated with higher recommendations that a male candidate should be employed as a manager. Benevolent sexism was unrelated to evaluations and recommendations in this context. The findings support the hypothesis that hostile, but not benevolent, sexism results in negativity toward individual women who pose a threat to men's status in the workplace.},
	number = {9},
	urldate = {2025-03-15},
	journal = {Sex Roles},
	author = {Masser, Barbara M. and Abrams, Dominic},
	month = nov,
	year = {2004},
	pages = {609--615},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\TBF33YXP\\Masser and Abrams - 2004 - Reinforcing the Glass Ceiling The Consequences of.pdf:application/pdf},
}

@article{gan_application_2024,
	title = {Application of {LLM} {Agents} in {Recruitment}: {A} {Novel} {Framework} for {Automated} {Resume} {Screening}},
	volume = {32},
	shorttitle = {Application of {LLM} {Agents} in {Recruitment}},
	doi = {10.2197/ipsjjip.32.881},
	abstract = {The automation of resume screening is a crucial aspect of the recruitment process in organizations. Automated resume screening systems often encompass a range of natural language processing (NLP) tasks. This paper introduces a novel Large Language Models (LLMs) based agent framework for resume screening, aimed at enhancing efficiency and time management in recruitment processes. Our framework is distinct in its ability to efficiently summarize and grade each resume from a large dataset. Moreover, it utilizes LLM agents for decision-making. To evaluate our framework, we constructed a dataset from actual resumes and simulated a resume screening process. Subsequently, the outcomes of the simulation experiment were compared and subjected to detailed analysis. The results demonstrate that our automated resume screening framework is 11 times faster than traditional manual methods. Furthermore, by fine-tuning the LLMs, we observed a significant improvement in the F1 score, reaching 87.73\%, during the resume sentence classification phase. In the resume summarization and grading phase, our fine-tuned model surpassed the baseline performance of the GPT-3.5 model. Analysis of the decision-making efficacy of the LLM agents in the final offer stage further underscores the potential of LLM agents in transforming resume screening processes.},
	journal = {Journal of Information Processing},
	author = {Gan, Chengguang and Zhang, Qinghao and Mori, Tatsunori},
	year = {2024},
	pages = {881--893},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\LWEPFBM8\\Gan et al. - 2024 - Application of LLM Agents in Recruitment A Novel .pdf:application/pdf;J-Stage - Snapshot:C\:\\Users\\jana\\Zotero\\storage\\8XZBWXRG\\ja.html:text/html},
}

@article{international_test_commission_international_2001,
	title = {International {Guidelines} for {Test} {Use}},
	volume = {1},
	issn = {1530-5058},
	url = {https://doi.org/10.1207/S15327574IJT0102_1},
	doi = {10.1207/S15327574IJT0102_1},
	abstract = {Developed by the International Test Commission (ITC), the International Guidelines for Test Use are a set of guidelines that provide an international view on areas of consensus on what constitutes "good practice" in test use, without being prescriptive about how these guidelines should be implemented by national professional psychological associations and other organizations associated with testing. In addition to key competencies (including knowledge, skills, and other personal and contextual factors) needed for responsible test use, the Guidelines also address issues of professional and ethical standards in testing; rights of the test taker and other parties involved in the testing process; choice and evaluation of alternative tests; test administration, scoring, interpretation, report writing and feedback. Appendixes cover guidelines for an outline policy on testing; guidelines for developing contracts between parties involved in the testing process; and points to consider when making arrangements for testing people with disabilities or impairments.},
	number = {2},
	urldate = {2025-03-17},
	journal = {International Journal of Testing},
	author = {{International Test Commission}},
	month = jun,
	year = {2001},
	pages = {93--114},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\AMQIHRGD\\Commission - 2001 - International Guidelines for Test Use.pdf:application/pdf},
}

@misc{apa_dictionary_of_psychology_psychological_2018,
	title = {Psychological test},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-17},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\M2IT9FKE\\psychological-test.html:text/html},
}

@book{american_educational_research_association_standards_2014,
	address = {Washington, DC, USA},
	title = {Standards for educational and psychological testing},
	isbn = {978-0-935302-35-6},
	publisher = {American Educational Research Association},
	author = {{American Educational Research Association} and {American Psychological Association} and {National Council on Measurement in Education}},
	year = {2014},
	file = {American Educational Research Association et al. - 2014 - Standards for educational and psychological testin.pdf:C\:\\Users\\jana\\Zotero\\storage\\UB9L9ZSS\\American Educational Research Association et al. - 2014 - Standards for educational and psychological testin.pdf:application/pdf},
}

@misc{apa_dictionary_of_psychology_validity_2018,
	title = {Validity},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-17},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\LMDMWMNG\\validity.html:text/html},
}

@article{kane_validating_2013,
	title = {Validating the {Interpretations} and {Uses} of {Test} {Scores}},
	volume = {50},
	copyright = {Copyright {\textcopyright} 2013 by the National Council on Measurement in Education},
	issn = {1745-3984},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jedm.12000},
	doi = {10.1111/jedm.12000},
	abstract = {To validate an interpretation or use of test scores is to evaluate the plausibility of the claims based on the scores. An argument-based approach to validation suggests that the claims based on the test scores be outlined as an argument that specifies the inferences and supporting assumptions needed to get from test responses to score-based interpretations and uses. Validation then can be thought of as an evaluation of the coherence and completeness of this interpretation/use argument and of the plausibility of its inferences and assumptions. In outlining the argument-based approach to validation, this paper makes eight general points. First, it is the proposed score interpretations and uses that are validated and not the test or the test scores. Second, the validity of a proposed interpretation or use depends on how well the evidence supports the claims being made. Third, more-ambitious claims require more support than less-ambitious claims. Fourth, more-ambitious claims (e.g., construct interpretations) tend to be more useful than less-ambitious claims, but they are also harder to validate. Fifth, interpretations and uses can change over time in response to new needs and new understandings leading to changes in the evidence needed for validation. Sixth, the evaluation of score uses requires an evaluation of the consequences of the proposed uses; negative consequences can render a score use unacceptable. Seventh, the rejection of a score use does not necessarily invalidate a prior, underlying score interpretation. Eighth, the validation of the score interpretation on which a score use is based does not validate the score use.},
	number = {1},
	urldate = {2025-03-18},
	journal = {Journal of Educational Measurement},
	author = {Kane, Michael T.},
	year = {2013},
	pages = {1--73},
	file = {Kane - 2013 - Validating the Interpretations and Uses of Test Sc.pdf:C\:\\Users\\jana\\Zotero\\storage\\VVZXIEJ2\\Kane - 2013 - Validating the Interpretations and Uses of Test Sc.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\Y6GWC3DG\\jedm.html:text/html},
}

@misc{apa_dictionary_of_psychology_reliability_2018,
	title = {Reliability},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-18},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\VNCY5HBJ\\reliability.html:text/html},
}

@misc{apa_dictionary_of_psychology_internal_2018,
	title = {Internal consistency},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-18},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\ZYLSH8AV\\internal-consistency.html:text/html},
}

@misc{apa_dictionary_of_psychology_alternate_2018,
	title = {Alternate form},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-18},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\A6252UE6\\alternate-form.html:text/html},
}

@article{cronbach_construct_1955,
	title = {Construct validity in psychological tests},
	volume = {52},
	issn = {1939-1455},
	doi = {10.1037/h0040957},
	abstract = {"Construct validation was introduced in order to specify types of research required in developing tests for which the conventional views on validation are inappropriate. Personality tests, and some tests of ability, are interpreted in terms of attributes for which there is no adequate criterion. This paper indicates what sorts of evidence can substantiate such an interpretation, and how such evidence is to be interpreted." 60 references. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	number = {4},
	journal = {Psychological Bulletin},
	author = {Cronbach, Lee J. and Meehl, Paul E.},
	year = {1955},
	pages = {281--302},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\VRDYW49Z\\1956-03730-001.html:text/html;Submitted Version:C\:\\Users\\jana\\Zotero\\storage\\EAFBGDEM\\Cronbach and Meehl - 1955 - Construct validity in psychological tests.pdf:application/pdf},
}

@article{binz_using_2023,
	title = {Using cognitive psychology to understand {GPT}-3},
	volume = {120},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2218523120},
	doi = {10.1073/pnas.2218523120},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3{\textquoteright}s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3{\textquoteright}s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	number = {6},
	urldate = {2025-03-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Binz, Marcel and Schulz, Eric},
	month = feb,
	year = {2023},
	keywords = {machine psychology, individual},
	pages = {e2218523120},
	file = {Full Text PDF:C\:\\Users\\jana\\Zotero\\storage\\RKCFUAD8\\Binz and Schulz - 2023 - Using cognitive psychology to understand GPT-3.pdf:application/pdf},
}

@misc{xu_expertprompting_2025,
	title = {{ExpertPrompting}: {Instructing} {Large} {Language} {Models} to be {Distinguished} {Experts}},
	shorttitle = {{ExpertPrompting}},
	url = {http://arxiv.org/abs/2305.14688},
	doi = {10.48550/arXiv.2305.14688},
	abstract = {The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96{\textbackslash}\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at https://github.com/OFA-Sys/ExpertLLaMA.},
	urldate = {2025-03-19},
	publisher = {arXiv},
	author = {Xu, Benfeng and Yang, An and Lin, Junyang and Wang, Quan and Zhou, Chang and Zhang, Yongdong and Mao, Zhendong},
	month = mar,
	year = {2025},
	file = {Preprint PDF:C\:\\Users\\jana\\Zotero\\storage\\ABS669V9\\Xu et al. - 2025 - ExpertPrompting Instructing Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\jana\\Zotero\\storage\\BRBNML6R\\2305.html:text/html},
}

@article{cronbach_alpha_1965,
	title = {Alpha {Coefficients} for {Stratified}-{Parallel} {Tests}},
	volume = {25},
	issn = {0013-1644},
	url = {https://doi.org/10.1177/001316446502500201},
	doi = {10.1177/001316446502500201},
	number = {2},
	urldate = {2025-03-21},
	journal = {Educational and Psychological Measurement},
	author = {Cronbach, Lee J. and Sch{\"o}nemann, Peter and McKie, Douglas},
	month = jul,
	year = {1965},
	pages = {291--312},
	file = {SAGE PDF Full Text:C\:\\Users\\jana\\Zotero\\storage\\XIAJBPXG\\Cronbach et al. - 1965 - Alpha Coefficients for Stratified-Parallel Tests.pdf:application/pdf},
}

@book{meyer_reliability_2010,
	address = {New York; Oxford},
	series = {Series in understanding statistics. {Measurement}},
	title = {Reliability},
	isbn = {978-0-19-984791-4},
	abstract = {From the 'Understanding Statistics series', this book addresses reliability, which is a fundamental aspect of any social science study that involves educational or psychological measurement. It not only has implications for the quality of test scores, but also for any analysis using those scores},
	publisher = {Oxford University Press},
	author = {Meyer, J. Patrick},
	year = {2010},
}

@article{cronbach_coefficient_1951,
	title = {Coefficient alpha and the internal structure of tests},
	volume = {16},
	number = {3},
	journal = {Psychometrika},
	author = {Cronbach, Lee J.},
	year = {1951},
	pages = {297--334},
}

@article{villasenor_alva_generalization_2009,
	title = {A {Generalization} of {Shapiro-Wilk}'s {Test} for {Multivariate} {Normality}},
	volume = {38},
	issn = {0361-0926},
	url = {https://www.tandfonline.com/doi/full/10.1080/03610920802474465},
	doi = {10.1080/03610920802474465},
	number = {11},
	urldate = {2025-03-24},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Villasenor Alva, Jos{\'e} A. and Estrada, Elizabeth Gonz{\'a}lez},
	month = may,
	year = {2009},
	keywords = {Empirical standardization, Goodness-of-fit test, Henze and Zirkler's test, Mardia's tests, Monte Carlo simulation, Primary 62G10, Secondary 62Q05, Standard normal distribution},
	pages = {1870--1883},
	file = {Villasenor Alva and and Estrada - 2009 - A Generalization of Shapiro{\textendash}Wilk's Test for Multiv.pdf:C\:\\Users\\jana\\Zotero\\storage\\A2R3ZHU3\\Villasenor Alva and and Estrada - 2009 - A Generalization of Shapiro{\textendash}Wilk's Test for Multiv.pdf:application/pdf},
}

@book{byrne_structural_1994,
	address = {Los Angeles, California USA},
	title = {Structural {Equation} {Modeling} with {EQS} and {EQS}/{WINDOWS}: {Basic} {Concepts}, {Applications}, and {Programming}},
	isbn = {978-0-8039-5092-4},
	abstract = {"The strongest attribute of Barbara M. Byrne's book is her superb organization both across chapters and within chapters. . . . Aside from the logical layout of chapters, a great deal of helpful organization exists on many other levels. After making readers comfortable with the details of the input and output files, Byrne gradually weans us away from the masses of information that would otherwise be redundant; she does, however, remind us of key points from time to time, almost anticipating readers' questions. . . . Organization within chapters is also quite sound. . . . Her EQS input files are neatly structured so that indicators of a common factor are indented to the same degree. . . . All application chapters nicely conclude with references to other examples of the application under study. . . . As should be obvious by now, I was fairly pleased with what I found in Byrne's book. . . . As the book unfolds she does a nice job of addressing many topics within the context of her applications. . . . It has strong value as a tool for teachers and practitioners." --Gregory R. Hancock in Structural Equation Modeling "Several features of the illustrations will be beneficial to SEM novices. The EQS programming and interpretation are presented clearly and in detail so that readers should have little trouble applying the concepts in their own research. An especially nice feature is that, as is usual with ''real'' data, things ''go wrong.'' . . . Seeing realistic rather than idealistic applications is good preparation for practicing and interpreting SEM. . . . The major benefit of the books to experienced modelers who are new to EQS is the convenience of having detailed examples to follow. None of Barbara M. Byrne's applications involve ''tricks'' or steps that would not be clear from the EQS and EQS/Windows manuals. . . . Byrne's step-by-step detail and integrated discussion of data screening and model respecification can reduce the time spent fixing minor errors in program setups or searching through the manuals for desired options. . . . The book is a fine production. . . . Newcomers to SEM who want to use EQS or EQS/Windows would benefit greatly from this book, either for self-teaching or as a supplemental text in a course. Experienced modelers will benefit less, but spending a few hours with the book will move them well along the EQS learning curve. Byrne intends her books ''to provide a practical guide to SEM using the EQS approach.'' She succeeds admirably." --George R. Franke in Journal of Marketing Research "For each chapter, the author has been very successful in conveying complicated concepts involved in structural equation modeling with explanations that are easy to understand for users with little background in SEM. The examples used in this book represent a broad range of research topics in the area of SEM. The author also provides precise linkage between the null hypothesis to be tested, the specific setup of EQS programs, and clear explanations of the results. Beginners of SEM can benefit substantially from this down-to-earth approach." --Chih-Ping Chou, University of Southern California "Byrne's book is a delightfully written, systematic approach to the practical use of structural equation modeling in research. It begins with first principles and advances systematically through intermediate and advanced topics, introducing at each step the theory that is relevant as well as the practical models needed to implement these ideas on real models used in actual research contexts. The reader can then easily modify the examples for their own research since the types of models discussed will be relevant in many research contexts. Mathematical and statistical details are minimized so that the book should appeal to a broad audience. The book is also timely, being the first text to show how statistics generally, and modeling in particular, can be fun in the Windows environment." --Peter M. Bentler, University of California, Los Angeles Researchers and students who want a less technical alternative to the EQS manual will find exactly what they're looking for in this practical volume. Designed to help beginners estimate and test structural equation modeling (SEM) using the EQS approach, this practical volume clearly explains and demonstrates a wide variety of SEM/EQS applications that include both partial factor analytic and full latent variable models. Beginning with an overview of the basic concepts of SEM and the EQS program, the author carefully works through applications starting with the easiest, a single sample approach, through to more advanced applications, such as a multi-sample approach. The book concludes with a section on using EQS for modeling with Windows.},
	publisher = {Sage},
	author = {Byrne, Barbara M.},
	month = feb,
	year = {1994},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Reference / Research, Social Science / Research, Social Science / Statistics},
}

@article{rosseel_lavaan_2012,
	title = {lavaan: {An} {R} {Package} for {Structural} {Equation} {Modeling}},
	volume = {48},
	copyright = {Copyright (c) 2011 Yves Rosseel},
	issn = {1548-7660},
	shorttitle = {lavaan},
	url = {https://doi.org/10.18637/jss.v048.i02},
	doi = {10.18637/jss.v048.i02},
	abstract = {Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. Over the years, many software packages for structural equation modeling have been developed, both free and commercial. However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice.},
	urldate = {2025-03-29},
	journal = {Journal of Statistical Software},
	author = {Rosseel, Yves},
	month = may,
	year = {2012},
	pages = {1--36},
}

@misc{apa_dictionary_of_psychology_representative_2023,
	title = {Representative sampling},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-31},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = nov,
	year = {2023},
	file = {Snapshot:C\:\\Users\\jana\\Zotero\\storage\\PLKZE999\\representative-sampling.html:text/html},
}

@misc{apa_dictionary_of_psychology_identity_2018,
	title = {Identity},
	url = {https://dictionary.apa.org/},
	abstract = {A trusted reference in the field of psychology, offering more than 25,000 clear and authoritative entries.},
	urldate = {2025-03-31},
	journal = {APA Dictionary of Psychology},
	author = {{APA Dictionary of Psychology}},
	month = apr,
	year = {2018},
}

@incollection{glick_ambivalent_2001-1,
	address = {San Diego, California, USA},
	title = {Ambivalent sexism},
	volume = {33},
	isbn = {978-0-12-015233-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065260101800058},
	urldate = {2025-04-03},
	booktitle = {Advances in {Experimental} {Social} {Psychology}},
	publisher = {Academic Press},
	author = {Glick, Peter and Fiske, Susan T.},
	year = {2001},
	pages = {115--188},
}
